{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "1_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WDdC5E0llYu",
        "outputId": "a29254d3-119d-4cda-9f2a-e8d0f0ac9c6a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XQHea_TnP4S",
        "outputId": "2f1ebdca-2b10-404d-ea4a-51a85bcc0e71"
      },
      "source": [
        "# Install idx2numpy package for extracting data\n",
        "!pip install idx2numpy"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: idx2numpy in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9CsmgA1jCg4"
      },
      "source": [
        "# Import packages\n",
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "import idx2numpy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmfcKJJCjCg5"
      },
      "source": [
        "def load_one_dataset(path):\n",
        "    '''\n",
        "    Convenience function to load a single dataset\n",
        "    '''\n",
        "    f = gzip.open(path, 'rb')\n",
        "    data = torch.from_numpy(idx2numpy.convert_from_file(f))\n",
        "    f.close()\n",
        "    \n",
        "    return(data)\n",
        "\n",
        "\n",
        "def load_all_datasets(train_imgs, train_labs, test_imgs, test_labs, batch_size):\n",
        "    '''\n",
        "    Load training as well as test images here\n",
        "    '''\n",
        "    train_images = load_one_dataset(train_imgs).type(torch.float32)\n",
        "    train_labels = load_one_dataset(train_labs).type(torch.long)\n",
        "    train = list(zip(train_images, train_labels))\n",
        "\n",
        "    test_images = load_one_dataset(test_imgs).type(torch.float32)\n",
        "    test_labels = load_one_dataset(test_labs).type(torch.long)\n",
        "    test = list(zip(test_images, test_labels))\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    return(train_loader, test_loader)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxKAhnC9rieJ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  \n",
        "  def __init__(self, nb_units, input_dim, output_dim):\n",
        "    '''\n",
        "    Declare the network architecture here\n",
        "    '''\n",
        "    super(Net, self).__init__()\n",
        "    \n",
        "    # Initialize a list to store layers\n",
        "    fc = []\n",
        "\n",
        "    # Add input and output dimensions to layer list\n",
        "    self.nb_units = [input_dim] + nb_units + [output_dim]\n",
        "\n",
        "    # Now compute the total no. of layers\n",
        "    self.nb_layers = len(self.nb_units)\n",
        "\n",
        "    # Now append the hidden layers\n",
        "    for i in range(1, self.nb_layers):\n",
        "      fc.append(nn.Linear(self.nb_units[i-1], self.nb_units[i]))\n",
        "    \n",
        "    # Wrap this in a module list \n",
        "    self.fc = nn.ModuleList(fc)\n",
        "    \n",
        "  \n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Send input forward through \n",
        "    the network\n",
        "    '''\n",
        "    # Reshape 28X28 images to be 784 X 784\n",
        "    x = x.view(-1, 28*28)\n",
        "\n",
        "    # Send example through network\n",
        "    for layer in self.fc: x = F.relu(layer(x))\n",
        "    \n",
        "    # Return statement\n",
        "    return x"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGe3H8TIjCg5"
      },
      "source": [
        "def train(config, input_dim=784, output_dim = 10, epochs=2,\n",
        "          data_dir = '/content/drive/MyDrive/data'):\n",
        "    '''\n",
        "    This is the main training loop\n",
        "    '''\n",
        "    # Set device\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "    \n",
        "    # Set paths to datasets\n",
        "    paths = {\n",
        "        'train_imgs': os.path.join(data_dir, 'train-images-idx3-ubyte.gz'),\n",
        "        'train_labs': os.path.join(data_dir, 'train-labels-idx1-ubyte.gz'),\n",
        "        'test_imgs': os.path.join(data_dir,'t10k-images-idx3-ubyte.gz'),\n",
        "        'test_labs': os.path.join(data_dir,'t10k-labels-idx1-ubyte.gz')\n",
        "    }\n",
        "\n",
        "    # Load datasets\n",
        "    train_loader, test_loader = load_all_datasets(**paths, batch_size = config['batch_size'])\n",
        "    \n",
        "    # Set parameters\n",
        "    net = Net(config['nb_units'], input_dim, output_dim)\n",
        "    \n",
        "    # Send net object to device memory\n",
        "    net.to(device)\n",
        "    \n",
        "    # We use the cross-entropy loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # We use mini-batch stochastic gradient descent with momentum\n",
        "    optimizer = optim.SGD(net.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
        "\n",
        "    # Store results here\n",
        "    results = {\n",
        "      'train_loss': [], \n",
        "      'train_accuracy': [],\n",
        "      'val_loss': [], \n",
        "      'val_accuracy': []\n",
        "      }\n",
        "\n",
        "    # Loop over the dataset multiple times\n",
        "    for epoch in range(epochs):  \n",
        "        \n",
        "        # Initialize running loss\n",
        "        running_loss = 0.0\n",
        "        running_accuracy = 0.0\n",
        "\n",
        "        # Initialize the validation running loss\n",
        "        val_running_loss = 0.0\n",
        "        val_running_accuracy = 0.0\n",
        "        \n",
        "        # Iterate through data now\n",
        "        for i, data in enumerate(train_loader):\n",
        "            \n",
        "            # Get the inputs: data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            \n",
        "            # Send the inputs to the memory of the device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            \n",
        "            # Optimize\n",
        "            optimizer.step()\n",
        "\n",
        "            # Add to running loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Add to running accuracy\n",
        "            running_accuracy += (preds == labels).float().sum()\n",
        "        \n",
        "        # Loop through the validation data\n",
        "        for j, data in enumerate(test_loader):\n",
        "          \n",
        "          # No need to calculate gradients for validation set\n",
        "          with torch.no_grad():\n",
        "\n",
        "              # Get the data item \n",
        "              val_inputs, val_labels = data\n",
        "              val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "\n",
        "              # Send the data item through the network to get output\n",
        "              val_outputs = net(val_inputs)\n",
        "\n",
        "              # Compute the loss\n",
        "              val_loss = criterion(val_outputs, val_labels)\n",
        "\n",
        "              # Get predictions\n",
        "              _, val_preds = torch.max(val_outputs.data, 1)\n",
        "\n",
        "              # Add to running loss\n",
        "              val_running_loss += val_loss.item()\n",
        "\n",
        "              # Add to running accuracy\n",
        "              val_running_accuracy += (val_preds == val_labels).float().sum()\n",
        "        \n",
        "        # Rescale the training and validation perfomance metrics\n",
        "        running_loss = running_loss/len(train_loader)\n",
        "        running_accuracy = running_accuracy/(len(train_loader)*config['batch_size'])\n",
        "        \n",
        "        # Rescale the validation loss\n",
        "        val_running_loss = val_running_loss/len(test_loader)\n",
        "        val_running_accuracy = val_running_accuracy/(len(test_loader)*config['batch_size'])\n",
        "        \n",
        "        # Append to the results tracker\n",
        "        results['train_loss'].append(np.float(running_loss))\n",
        "        results['train_accuracy'].append(np.float(running_accuracy))\n",
        "        results['val_loss'].append(np.float(val_running_loss))\n",
        "        results['val_accuracy'].append(np.float(val_running_accuracy))\n",
        "\n",
        "        # Make print message format string\n",
        "        msg = \"{}, Epoch:{}, Loss:{}, Accuracy:{},\" \"\\n\"\n",
        "\n",
        "        # Print performance\n",
        "        print(msg.format(\"Training\", epoch, running_loss, running_accuracy))\n",
        "        print(msg.format(\"Validation\", epoch, val_running_loss, val_running_accuracy))\n",
        "        \n",
        "    # Print message\n",
        "    print('Done training...')\n",
        "    \n",
        "    # Return statement\n",
        "    return(results)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lknQEmr1wgiF"
      },
      "source": [
        "def get_random_hyper_parameters(n_layers=4):\n",
        "    '''\n",
        "    Randomly select hyper-parameters to train with\n",
        "    '''\n",
        "    config = {\n",
        "        \n",
        "      'lr': np.random.uniform(1e-4, 1e-1),\n",
        "      'batch_size': 512,\n",
        "      'weight_decay': np.random.uniform(1e-3, 1e-1),\n",
        "      'nb_units': [2**np.random.randint(2,9) for i in range(n_layers)],\n",
        "      'momentum': np.random.choice([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
        "      'done': 0\n",
        "    }\n",
        "    \n",
        "    return(config)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RibzX_A2wFfu"
      },
      "source": [
        "def get_random_grid(n_samples=500, n_layers=4):\n",
        "  '''\n",
        "  Get a random set of hyper-parameter combinations\n",
        "  to test with for a given number of layers. \n",
        "  '''\n",
        "  \n",
        "  # Store the grid and results in this dictionary\n",
        "  experiment = {\n",
        "      \n",
        "      'params': [], \n",
        "      'results': []\n",
        "\n",
        "  }\n",
        "  \n",
        "  # This will generate the list of parameter combinations to search\n",
        "  for i in range(n_samples):   \n",
        "    config = get_random_hyper_parameters(n_layers)\n",
        "    experiment['params'].append(config)\n",
        "\n",
        "  # This is the experiment label\n",
        "  experiment_name = '{}_samples_{}_layers.json'.format(n_samples, n_layers)\n",
        "\n",
        "  # This is the experiment path\n",
        "  path = os.path.join(\"/content/drive/MyDrive/checkpoints\", experiment_name)\n",
        "\n",
        "  # We write the experiment dictionary as a .json file\n",
        "  with open(path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(experiment, f, ensure_ascii=False, indent=4)\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoFum6Ym6jm9"
      },
      "source": [
        "def run_grid_search(n_samples=100, n_layers=4, max_epochs=100):\n",
        "  '''\n",
        "  Run the grid search\n",
        "  '''\n",
        "\n",
        "  # First we load the experiment dictionary\n",
        "  experiment_name = '{}_samples_{}_layers.json'.format(n_samples, n_layers)\n",
        "  \n",
        "  # Create the complete path\n",
        "  path = os.path.join(\"/content/drive/MyDrive/checkpoints\", experiment_name)\n",
        "  \n",
        "  # Load the experiment data file\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    experiment = json.load(f)\n",
        "\n",
        "  # Loop through experiments data file\n",
        "  for i, config in enumerate(experiment['params']):\n",
        "    \n",
        "    # Print progress report\n",
        "    print(\"This is experiment {}\".format(i))\n",
        "\n",
        "    # Run experiments only for those parameter combinations which have not been tested\n",
        "    if config['done'] == 0:\n",
        "      results = train(config, epochs=max_epochs)\n",
        "      experiment['results'].append(results) \n",
        "      config['done'] = 1\n",
        "    \n",
        "    # We update the .json dictionary every 15 iterations\n",
        "    if i%15 == 0:\n",
        "      with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(experiment, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgBj-FezDwK4"
      },
      "source": [
        "# Create a new random grid if needed\n",
        "np.random.seed(129031908)\n",
        "new = 1\n",
        "num_samples = 200\n",
        "n_layers = 3\n",
        "if new == 1: \n",
        "  get_random_grid(num_samples, n_layers)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceYpawbIR1au",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b976c3f6-f4ef-4d15-c661-a3a9a50e75bd"
      },
      "source": [
        "# Run the grid search\n",
        "run_grid_search(num_samples, n_layers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is experiment 0\n",
            "Training, Epoch:0, Loss:478.9507746777292, Accuracy:0.09864936769008636,\n",
            "\n",
            "Validation, Epoch:0, Loss:2.303194487094879, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:1, Loss:2.303028242062714, Accuracy:0.09879833459854126,\n",
            "\n",
            "Validation, Epoch:1, Loss:2.302754592895508, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:2, Loss:2.3027376663886896, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:2, Loss:2.302633059024811, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:3, Loss:2.302641618049751, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:3, Loss:2.3025971055030823, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:4, Loss:2.3026158163103005, Accuracy:0.09881488233804703,\n",
            "\n",
            "Validation, Epoch:4, Loss:2.3025922894477846, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:5, Loss:2.302593109971386, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:5, Loss:2.302589166164398, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:6, Loss:2.3025915137792037, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:6, Loss:2.302585744857788, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:7, Loss:2.3025868706784007, Accuracy:0.0986328125,\n",
            "\n",
            "Validation, Epoch:7, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:8, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:8, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:9, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:9, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:10, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:10, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:11, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:11, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:12, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:12, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:13, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:13, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:14, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:14, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:15, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:15, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:16, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:16, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:17, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:17, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:18, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:18, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:19, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:19, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:20, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:20, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:21, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:21, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:22, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:22, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:23, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:23, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:24, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:24, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:25, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:25, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:26, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:26, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:27, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:27, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:28, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:28, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:29, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:29, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:30, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:30, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:31, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:31, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:32, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:32, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:33, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:33, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:34, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:34, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:35, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:35, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:36, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:36, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:37, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:37, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:38, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:38, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:39, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:39, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:40, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:40, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:41, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:41, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:42, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:42, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:43, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:43, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:44, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:44, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:45, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:45, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:46, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:46, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:47, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:47, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:48, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:48, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:49, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:49, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:50, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:50, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:51, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:51, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:52, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:52, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:53, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:53, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:54, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:54, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:55, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:55, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:56, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:56, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:57, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:57, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:58, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:58, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:59, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:59, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:60, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:60, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:61, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:61, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:62, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:62, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:63, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:63, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:64, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:64, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:65, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:65, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:66, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:66, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:67, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:67, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:68, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:68, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:69, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:69, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:70, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:70, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:71, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:71, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:72, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:72, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:73, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:73, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:74, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:74, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:75, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:75, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:76, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:76, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:77, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:77, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:78, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:78, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:79, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:79, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:80, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:80, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:81, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:81, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:82, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:82, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:83, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:83, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:84, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:84, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:85, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:85, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n",
            "Training, Epoch:86, Loss:2.302584894632889, Accuracy:0.09931144118309021,\n",
            "\n",
            "Validation, Epoch:86, Loss:2.302584874629974, Accuracy:0.09765625,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}