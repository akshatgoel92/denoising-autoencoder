{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "mlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOnHtZg1taNw"
      },
      "source": [
        "# To do\n",
        "# Test out variable layers\n",
        "# Add regularization\n",
        "# Add hyper-parameter tuning\n",
        "# Check everything\n",
        "# Run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XQHea_TnP4S",
        "outputId": "8e69c2d3-4056-478d-846b-ae90969c3f39"
      },
      "source": [
        "# Install idx2numpy package for extracting data\n",
        "!pip install idx2numpy"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting idx2numpy\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/6b/80628f6cc2f44d80b27f1ef7b57b257ed4c73766113b77d13ad110c091b4/idx2numpy-1.2.3.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.15.0)\n",
            "Building wheels for collected packages: idx2numpy\n",
            "  Building wheel for idx2numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idx2numpy: filename=idx2numpy-1.2.3-cp36-none-any.whl size=7905 sha256=6b0a91a3795f6a677eb5ba1236bb254fdd5c094547eda9b7c328d278d99e7478\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c1/da/284ce80a748fab898b8d1fa95468a386e7cf3b81da18511f9d\n",
            "Successfully built idx2numpy\n",
            "Installing collected packages: idx2numpy\n",
            "Successfully installed idx2numpy-1.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9CsmgA1jCg4"
      },
      "source": [
        "# Import packages\n",
        "import gzip\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np \n",
        "\n",
        "import idx2numpy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iQ0XWiQjyQs",
        "outputId": "24b15f5a-fdaf-4571-e29f-5122ba802afa"
      },
      "source": [
        "# Mount Google drive to access data from Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmfcKJJCjCg5"
      },
      "source": [
        "def load_one_dataset(path):\n",
        "    \n",
        "    f = gzip.open(path, 'rb')\n",
        "    data = torch.from_numpy(idx2numpy.convert_from_file(f))\n",
        "    f.close()\n",
        "    \n",
        "    return(data)\n",
        "\n",
        "\n",
        "def load_all_datasets(train_imgs, train_labs, test_imgs, test_labs, batch_size):\n",
        "    \n",
        "    \n",
        "    train_images = load_one_dataset(train_imgs).type(torch.float32)\n",
        "    train_labels = load_one_dataset(train_labs).type(torch.long)\n",
        "    train = list(zip(train_images, train_labels))\n",
        "    \n",
        "    test_images = load_one_dataset(test_imgs).type(torch.float32)\n",
        "    test_labels = load_one_dataset(test_labs).type(torch.long)\n",
        "    test = list(zip(test_images, test_labels))\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    return(train_loader, test_loader)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxKAhnC9rieJ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  \n",
        "  def __init__(self, nb_layers, nb_units, input_dim, output_dim):\n",
        "    '''\n",
        "    Layers are constructed here\n",
        "    '''\n",
        "    super(DNN, self).__init__()\n",
        "    self.nb_layers = nb_layers\n",
        "    fc = []\n",
        "    \n",
        "    for i in range(self.nb_layers):\n",
        "        if i == 0:\n",
        "            fc.append(nn.Linear(input_dim, nb_units[i]))\n",
        "        elif i == nb_layers-1:\n",
        "            fc.append(nn.Linear(nb_units[i], output_dim))\n",
        "        else:\n",
        "            fc.append(nn.Linear(nb_units[i], nb_units))\n",
        "    \n",
        "    self.fc = nn.ModuleList(fc)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Forward propagation happens here\n",
        "    '''\n",
        "    for i in range(self.nb_layers):\n",
        "      x = F.relu(self.fc[i](x))\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGe3H8TIjCg5"
      },
      "source": [
        "def train(nb_units=[256, 128, 64, 32, 16], input_dim=784, output_dim = 10, \n",
        "          epochs=2, lr=0.001, momentum=0.9, batch_size=512):\n",
        "    \n",
        "    \n",
        "    # Set paths to datasets\n",
        "    paths = {\n",
        "        \n",
        "        'train_imgs': '/content/gdrive/MyDrive/data/train-images-idx3-ubyte.gz',\n",
        "        'train_labs': '/content/gdrive/MyDrive/data/train-labels-idx1-ubyte.gz',\n",
        "        'test_imgs': '/content/gdrive/MyDrive/data/t10k-images-idx3-ubyte.gz',\n",
        "        'test_labs': '/content/gdrive/MyDrive/data/t10k-labels-idx1-ubyte.gz'\n",
        "    }\n",
        "    \n",
        "    # Load datasets\n",
        "    train_loader, test_loader = load_all_datasets(**paths, batch_size = batch_size)\n",
        "    \n",
        "    # Set parameters\n",
        "    net = Net()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "    \n",
        "    # Loop over the dataset multiple times\n",
        "    for epoch in range(epochs):  \n",
        "        \n",
        "        # Initialize running loss\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        # Iterate through data now\n",
        "        for i, data in enumerate(train_loader):\n",
        "            \n",
        "            # Get the inputs: data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            \n",
        "            # Optimize\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        # Initialize the validation running loss\n",
        "        val_running_loss = 0.0\n",
        "        \n",
        "        # No need to calculate gradients for validation set\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Loop through the validation data\n",
        "            for i, data in enumerate(test_loader):\n",
        "              \n",
        "              # Get the data item \n",
        "              val_inputs, val_labels = data\n",
        "\n",
        "              # Send the data item through the network to get output\n",
        "              val_outputs = net(val_inputs)\n",
        "\n",
        "              # Compute the loss\n",
        "              val_loss = criterion(val_outputs, val_labels)\n",
        "\n",
        "              # Add to the running validation loss\n",
        "              val_running_loss += val_loss.item()\n",
        "            \n",
        "        # Print every 100 mini-batches\n",
        "        print(\"The train loss on epoch {} is {}...\".format(epoch, running_loss))\n",
        "        print(\"The validation loss on epoch {} is {}...\".format(epoch, val_running_loss))\n",
        "    \n",
        "    # Print message\n",
        "    print('Done training...')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiK7qF9xjCg5",
        "outputId": "04162cca-7477-4f91-93ac-f5f8b45917eb"
      },
      "source": [
        "train(epochs=100)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The train loss on epoch 0 is 417.0383608341217...\n",
            "The validation loss on epoch 0 is 46.05197238922119...\n",
            "The train loss on epoch 1 is 271.70668840408325...\n",
            "The validation loss on epoch 1 is 46.05197238922119...\n",
            "The train loss on epoch 2 is 271.70668840408325...\n",
            "The validation loss on epoch 2 is 46.05197238922119...\n",
            "The train loss on epoch 3 is 271.70668840408325...\n",
            "The validation loss on epoch 3 is 46.05197238922119...\n",
            "The train loss on epoch 4 is 271.70668840408325...\n",
            "The validation loss on epoch 4 is 46.05197238922119...\n",
            "The train loss on epoch 5 is 271.70668840408325...\n",
            "The validation loss on epoch 5 is 46.05197238922119...\n",
            "The train loss on epoch 6 is 271.70668840408325...\n",
            "The validation loss on epoch 6 is 46.05197238922119...\n",
            "The train loss on epoch 7 is 271.70668840408325...\n",
            "The validation loss on epoch 7 is 46.05197238922119...\n",
            "The train loss on epoch 8 is 271.70668840408325...\n",
            "The validation loss on epoch 8 is 46.05197238922119...\n",
            "The train loss on epoch 9 is 271.70668840408325...\n",
            "The validation loss on epoch 9 is 46.05197238922119...\n",
            "The train loss on epoch 10 is 271.70668840408325...\n",
            "The validation loss on epoch 10 is 46.05197238922119...\n",
            "The train loss on epoch 11 is 271.70668840408325...\n",
            "The validation loss on epoch 11 is 46.05197238922119...\n",
            "The train loss on epoch 12 is 271.70668840408325...\n",
            "The validation loss on epoch 12 is 46.05197238922119...\n",
            "The train loss on epoch 13 is 271.70668840408325...\n",
            "The validation loss on epoch 13 is 46.05197238922119...\n",
            "The train loss on epoch 14 is 271.70668840408325...\n",
            "The validation loss on epoch 14 is 46.05197238922119...\n",
            "The train loss on epoch 15 is 271.70668840408325...\n",
            "The validation loss on epoch 15 is 46.05197238922119...\n",
            "The train loss on epoch 16 is 271.70668840408325...\n",
            "The validation loss on epoch 16 is 46.05197238922119...\n",
            "The train loss on epoch 17 is 271.70668840408325...\n",
            "The validation loss on epoch 17 is 46.05197238922119...\n",
            "The train loss on epoch 18 is 271.70668840408325...\n",
            "The validation loss on epoch 18 is 46.05197238922119...\n",
            "The train loss on epoch 19 is 271.70668840408325...\n",
            "The validation loss on epoch 19 is 46.05197238922119...\n",
            "The train loss on epoch 20 is 271.70668840408325...\n",
            "The validation loss on epoch 20 is 46.05197238922119...\n",
            "The train loss on epoch 21 is 271.70668840408325...\n",
            "The validation loss on epoch 21 is 46.05197238922119...\n",
            "The train loss on epoch 22 is 271.70668840408325...\n",
            "The validation loss on epoch 22 is 46.05197238922119...\n",
            "The train loss on epoch 23 is 271.70668840408325...\n",
            "The validation loss on epoch 23 is 46.05197238922119...\n",
            "The train loss on epoch 24 is 271.70668840408325...\n",
            "The validation loss on epoch 24 is 46.05197238922119...\n",
            "The train loss on epoch 25 is 271.70668840408325...\n",
            "The validation loss on epoch 25 is 46.05197238922119...\n",
            "The train loss on epoch 26 is 271.70668840408325...\n",
            "The validation loss on epoch 26 is 46.05197238922119...\n",
            "The train loss on epoch 27 is 271.70668840408325...\n",
            "The validation loss on epoch 27 is 46.05197238922119...\n",
            "The train loss on epoch 28 is 271.70668840408325...\n",
            "The validation loss on epoch 28 is 46.05197238922119...\n",
            "The train loss on epoch 29 is 271.70668840408325...\n",
            "The validation loss on epoch 29 is 46.05197238922119...\n",
            "The train loss on epoch 30 is 271.70668840408325...\n",
            "The validation loss on epoch 30 is 46.05197238922119...\n",
            "The train loss on epoch 31 is 271.70668840408325...\n",
            "The validation loss on epoch 31 is 46.05197238922119...\n",
            "The train loss on epoch 32 is 271.70668840408325...\n",
            "The validation loss on epoch 32 is 46.05197238922119...\n",
            "The train loss on epoch 33 is 271.70668840408325...\n",
            "The validation loss on epoch 33 is 46.05197238922119...\n",
            "The train loss on epoch 34 is 271.70668840408325...\n",
            "The validation loss on epoch 34 is 46.05197238922119...\n",
            "The train loss on epoch 35 is 271.70668840408325...\n",
            "The validation loss on epoch 35 is 46.05197238922119...\n",
            "The train loss on epoch 36 is 271.70668840408325...\n",
            "The validation loss on epoch 36 is 46.05197238922119...\n",
            "The train loss on epoch 37 is 271.70668840408325...\n",
            "The validation loss on epoch 37 is 46.05197238922119...\n",
            "The train loss on epoch 38 is 271.70668840408325...\n",
            "The validation loss on epoch 38 is 46.05197238922119...\n",
            "The train loss on epoch 39 is 271.70668840408325...\n",
            "The validation loss on epoch 39 is 46.05197238922119...\n",
            "The train loss on epoch 40 is 271.70668840408325...\n",
            "The validation loss on epoch 40 is 46.05197238922119...\n",
            "The train loss on epoch 41 is 271.70668840408325...\n",
            "The validation loss on epoch 41 is 46.05197238922119...\n",
            "The train loss on epoch 42 is 271.70668840408325...\n",
            "The validation loss on epoch 42 is 46.05197238922119...\n",
            "The train loss on epoch 43 is 271.70668840408325...\n",
            "The validation loss on epoch 43 is 46.05197238922119...\n",
            "The train loss on epoch 44 is 271.70668840408325...\n",
            "The validation loss on epoch 44 is 46.05197238922119...\n",
            "The train loss on epoch 45 is 271.70668840408325...\n",
            "The validation loss on epoch 45 is 46.05197238922119...\n",
            "The train loss on epoch 46 is 271.70668840408325...\n",
            "The validation loss on epoch 46 is 46.05197238922119...\n",
            "The train loss on epoch 47 is 271.70668840408325...\n",
            "The validation loss on epoch 47 is 46.05197238922119...\n",
            "The train loss on epoch 48 is 271.70668840408325...\n",
            "The validation loss on epoch 48 is 46.05197238922119...\n",
            "The train loss on epoch 49 is 271.70668840408325...\n",
            "The validation loss on epoch 49 is 46.05197238922119...\n",
            "The train loss on epoch 50 is 271.70668840408325...\n",
            "The validation loss on epoch 50 is 46.05197238922119...\n",
            "The train loss on epoch 51 is 271.70668840408325...\n",
            "The validation loss on epoch 51 is 46.05197238922119...\n",
            "The train loss on epoch 52 is 271.70668840408325...\n",
            "The validation loss on epoch 52 is 46.05197238922119...\n",
            "The train loss on epoch 53 is 271.70668840408325...\n",
            "The validation loss on epoch 53 is 46.05197238922119...\n",
            "The train loss on epoch 54 is 271.70668840408325...\n",
            "The validation loss on epoch 54 is 46.05197238922119...\n",
            "The train loss on epoch 55 is 271.70668840408325...\n",
            "The validation loss on epoch 55 is 46.05197238922119...\n",
            "The train loss on epoch 56 is 271.70668840408325...\n",
            "The validation loss on epoch 56 is 46.05197238922119...\n",
            "The train loss on epoch 57 is 271.70668840408325...\n",
            "The validation loss on epoch 57 is 46.05197238922119...\n",
            "The train loss on epoch 58 is 271.70668840408325...\n",
            "The validation loss on epoch 58 is 46.05197238922119...\n",
            "The train loss on epoch 59 is 271.70668840408325...\n",
            "The validation loss on epoch 59 is 46.05197238922119...\n",
            "The train loss on epoch 60 is 271.70668840408325...\n",
            "The validation loss on epoch 60 is 46.05197238922119...\n",
            "The train loss on epoch 61 is 271.70668840408325...\n",
            "The validation loss on epoch 61 is 46.05197238922119...\n",
            "The train loss on epoch 62 is 271.70668840408325...\n",
            "The validation loss on epoch 62 is 46.05197238922119...\n",
            "The train loss on epoch 63 is 271.70668840408325...\n",
            "The validation loss on epoch 63 is 46.05197238922119...\n",
            "The train loss on epoch 64 is 271.70668840408325...\n",
            "The validation loss on epoch 64 is 46.05197238922119...\n",
            "The train loss on epoch 65 is 271.70668840408325...\n",
            "The validation loss on epoch 65 is 46.05197238922119...\n",
            "The train loss on epoch 66 is 271.70668840408325...\n",
            "The validation loss on epoch 66 is 46.05197238922119...\n",
            "The train loss on epoch 67 is 271.70668840408325...\n",
            "The validation loss on epoch 67 is 46.05197238922119...\n",
            "The train loss on epoch 68 is 271.70668840408325...\n",
            "The validation loss on epoch 68 is 46.05197238922119...\n",
            "The train loss on epoch 69 is 271.70668840408325...\n",
            "The validation loss on epoch 69 is 46.05197238922119...\n",
            "The train loss on epoch 70 is 271.70668840408325...\n",
            "The validation loss on epoch 70 is 46.05197238922119...\n",
            "The train loss on epoch 71 is 271.70668840408325...\n",
            "The validation loss on epoch 71 is 46.05197238922119...\n",
            "The train loss on epoch 72 is 271.70668840408325...\n",
            "The validation loss on epoch 72 is 46.05197238922119...\n",
            "The train loss on epoch 73 is 271.70668840408325...\n",
            "The validation loss on epoch 73 is 46.05197238922119...\n",
            "The train loss on epoch 74 is 271.70668840408325...\n",
            "The validation loss on epoch 74 is 46.05197238922119...\n",
            "The train loss on epoch 75 is 271.70668840408325...\n",
            "The validation loss on epoch 75 is 46.05197238922119...\n",
            "The train loss on epoch 76 is 271.70668840408325...\n",
            "The validation loss on epoch 76 is 46.05197238922119...\n",
            "The train loss on epoch 77 is 271.70668840408325...\n",
            "The validation loss on epoch 77 is 46.05197238922119...\n",
            "The train loss on epoch 78 is 271.70668840408325...\n",
            "The validation loss on epoch 78 is 46.05197238922119...\n",
            "The train loss on epoch 79 is 271.70668840408325...\n",
            "The validation loss on epoch 79 is 46.05197238922119...\n",
            "The train loss on epoch 80 is 271.70668840408325...\n",
            "The validation loss on epoch 80 is 46.05197238922119...\n",
            "The train loss on epoch 81 is 271.70668840408325...\n",
            "The validation loss on epoch 81 is 46.05197238922119...\n",
            "The train loss on epoch 82 is 271.70668840408325...\n",
            "The validation loss on epoch 82 is 46.05197238922119...\n",
            "The train loss on epoch 83 is 271.70668840408325...\n",
            "The validation loss on epoch 83 is 46.05197238922119...\n",
            "The train loss on epoch 84 is 271.70668840408325...\n",
            "The validation loss on epoch 84 is 46.05197238922119...\n",
            "The train loss on epoch 85 is 271.70668840408325...\n",
            "The validation loss on epoch 85 is 46.05197238922119...\n",
            "The train loss on epoch 86 is 271.70668840408325...\n",
            "The validation loss on epoch 86 is 46.05197238922119...\n",
            "The train loss on epoch 87 is 271.70668840408325...\n",
            "The validation loss on epoch 87 is 46.05197238922119...\n",
            "The train loss on epoch 88 is 271.70668840408325...\n",
            "The validation loss on epoch 88 is 46.05197238922119...\n",
            "The train loss on epoch 89 is 271.70668840408325...\n",
            "The validation loss on epoch 89 is 46.05197238922119...\n",
            "The train loss on epoch 90 is 271.70668840408325...\n",
            "The validation loss on epoch 90 is 46.05197238922119...\n",
            "The train loss on epoch 91 is 271.70668840408325...\n",
            "The validation loss on epoch 91 is 46.05197238922119...\n",
            "The train loss on epoch 92 is 271.70668840408325...\n",
            "The validation loss on epoch 92 is 46.05197238922119...\n",
            "The train loss on epoch 93 is 271.70668840408325...\n",
            "The validation loss on epoch 93 is 46.05197238922119...\n",
            "The train loss on epoch 94 is 271.70668840408325...\n",
            "The validation loss on epoch 94 is 46.05197238922119...\n",
            "The train loss on epoch 95 is 271.70668840408325...\n",
            "The validation loss on epoch 95 is 46.05197238922119...\n",
            "The train loss on epoch 96 is 271.70668840408325...\n",
            "The validation loss on epoch 96 is 46.05197238922119...\n",
            "The train loss on epoch 97 is 271.70668840408325...\n",
            "The validation loss on epoch 97 is 46.05197238922119...\n",
            "The train loss on epoch 98 is 271.70668840408325...\n",
            "The validation loss on epoch 98 is 46.05197238922119...\n",
            "The train loss on epoch 99 is 271.70668840408325...\n",
            "The validation loss on epoch 99 is 46.05197238922119...\n",
            "Done training...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}