{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "mlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WDdC5E0llYu",
        "outputId": "9d46f861-1e68-465e-874d-dca36f6d1d31"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XQHea_TnP4S",
        "outputId": "f4ecf071-6cf2-4230-cabc-4ec7dda2d9d8"
      },
      "source": [
        "# Install idx2numpy package for extracting data\n",
        "!pip install idx2numpy ray"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: idx2numpy in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.6/dist-packages (1.0.1.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray) (1.33.2)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray) (0.9.0)\n",
            "Requirement already satisfied: aioredis in /usr/local/lib/python3.6/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray) (2.6.0)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray) (2.0.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ray) (0.4.4)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.6/dist-packages (from ray) (0.7.11)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray) (0.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray) (3.7.3)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.6/dist-packages (from ray) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray) (3.0.12)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray) (3.12.4)\n",
            "Requirement already satisfied: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray) (3.4.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray) (1.0.0)\n",
            "Requirement already satisfied: gpustat in /usr/local/lib/python3.6/dist-packages (from ray) (0.6.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.6/dist-packages (from ray) (0.5.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: async-timeout in /usr/local/lib/python3.6/dist-packages (from aioredis->ray) (3.0.1)\n",
            "Requirement already satisfied: hiredis in /usr/local/lib/python3.6/dist-packages (from aioredis->ray) (1.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray) (4.6.3)\n",
            "Requirement already satisfied: opencensus-context==0.1.2 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray) (0.1.2)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (20.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (1.6.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (5.0.2)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray) (50.3.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from gpustat->ray) (5.4.8)\n",
            "Requirement already satisfied: blessings>=1.6 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray) (1.7)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray) (7.352.0)\n",
            "Requirement already satisfied: contextvars; python_version >= \"3.6\" and python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from opencensus-context==0.1.2->opencensus->ray) (2.4)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.17.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (2018.9)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version >= \"3.6\" and python_version < \"3.7\"->opencensus-context==0.1.2->opencensus->ray) (0.14)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9CsmgA1jCg4"
      },
      "source": [
        "# Import packages\n",
        "import os\n",
        "import gzip\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np \n",
        "\n",
        "import idx2numpy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6rSib9JBEx2"
      },
      "source": [
        "from ray import tune\n",
        "# Import packages\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from functools import partial"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmfcKJJCjCg5"
      },
      "source": [
        "def load_one_dataset(path):\n",
        "    '''\n",
        "    Convenience function to load a single dataset\n",
        "    '''\n",
        "    f = gzip.open(path, 'rb')\n",
        "    data = torch.from_numpy(idx2numpy.convert_from_file(f))\n",
        "    f.close()\n",
        "    \n",
        "    return(data)\n",
        "\n",
        "\n",
        "def load_all_datasets(train_imgs, train_labs, test_imgs, test_labs, batch_size):\n",
        "    '''\n",
        "    Load training as well as test images here\n",
        "    '''\n",
        "    train_images = load_one_dataset(train_imgs).type(torch.float32)\n",
        "    train_labels = load_one_dataset(train_labs).type(torch.long)\n",
        "    train = list(zip(train_images, train_labels))\n",
        "\n",
        "    test_images = load_one_dataset(test_imgs).type(torch.float32)\n",
        "    test_labels = load_one_dataset(test_labs).type(torch.long)\n",
        "    test = list(zip(test_images, test_labels))\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    return(train_loader, test_loader)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxKAhnC9rieJ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  \n",
        "  def __init__(self, nb_units, input_dim, output_dim):\n",
        "    '''\n",
        "    Declare the network architecture here\n",
        "    '''\n",
        "    super(Net, self).__init__()\n",
        "    \n",
        "    # Initialize a list to store layers\n",
        "    fc = []\n",
        "\n",
        "    # Add input and output dimensions to layer list\n",
        "    self.nb_units = [input_dim] + nb_units + [output_dim]\n",
        "\n",
        "    # Now compute the total no. of layers\n",
        "    self.nb_layers = len(self.nb_units)\n",
        "\n",
        "    # Now append the hidden layers\n",
        "    for i in range(1, self.nb_layers):\n",
        "      fc.append(nn.Linear(self.nb_units[i-1], self.nb_units[i]))\n",
        "    \n",
        "    # Wrap this in a module list \n",
        "    self.fc = nn.ModuleList(fc)\n",
        "    \n",
        "  \n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Send input forward through \n",
        "    the network\n",
        "    '''\n",
        "    # Reshape 28X28 images to be 784 X 784\n",
        "    x = x.view(-1, 28*28)\n",
        "\n",
        "    # Send example through network\n",
        "    for layer in self.fc: x = F.relu(layer(x))\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGe3H8TIjCg5"
      },
      "source": [
        "def train(config, input_dim=784, output_dim = 10, epochs=2, \n",
        "          checkpoint_dir = '/content/drive/MyDrive/checkpoints/',\n",
        "          data_dir = '/content/drive/MyDrive/data/'):\n",
        "    '''\n",
        "    This is the main training loop\n",
        "    '''\n",
        "    \n",
        "    # Set paths to datasets\n",
        "    paths = {\n",
        "        'train_imgs': os.path.join(data_dir, 'train-images-idx3-ubyte.gz'),\n",
        "        'train_labs': os.path.join(data_dir, 'train-labels-idx1-ubyte.gz'),\n",
        "        'test_imgs': os.path.join(data_dir,'t10k-images-idx3-ubyte.gz'),\n",
        "        'test_labs': os.path.join(data_dir,'t10k-labels-idx1-ubyte.gz')\n",
        "    }\n",
        "    \n",
        "    # Load datasets\n",
        "    train_loader, test_loader = load_all_datasets(**paths, batch_size = config['batch_size'])\n",
        "    \n",
        "    # Set parameters\n",
        "    net = Net(config['nb_units'], input_dim, output_dim)\n",
        "\n",
        "    # Set device\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "    \n",
        "    # Send net object to device memory\n",
        "    net.to(device)\n",
        "    \n",
        "    # We use the cross-entropy loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # We use mini-batch stochastic gradient descent with momentum\n",
        "    optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "\n",
        "    # Load from previously stored results if specified\n",
        "    if checkpoint_dir:\n",
        "      model_state, optimizer_state = torch.load(os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "      net.load_state_dict(model_state)\n",
        "      optimizer.load_state_dict(optimizer_state)\n",
        "    \n",
        "    # Loop over the dataset multiple times\n",
        "    for epoch in range(epochs):  \n",
        "        \n",
        "        # Initialize running loss\n",
        "        running_loss = 0.0\n",
        "        running_accuracy = 0.0\n",
        "\n",
        "        # Initialize the validation running loss\n",
        "        val_running_loss = 0.0\n",
        "        val_running_accuracy = 0.0\n",
        "        \n",
        "        # Iterate through data now\n",
        "        for i, data in enumerate(train_loader):\n",
        "            \n",
        "            # Get the inputs: data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            \n",
        "            # Send the inputs to the memory of the device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            \n",
        "            # Optimize\n",
        "            optimizer.step()\n",
        "\n",
        "            # Add to running loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Add to running accuracy\n",
        "            running_accuracy += (preds == labels).float().sum()\n",
        "        \n",
        "        # Loop through the validation data\n",
        "        for j, data in enumerate(test_loader):\n",
        "          \n",
        "          # No need to calculate gradients for validation set\n",
        "          with torch.no_grad():\n",
        "\n",
        "              # Get the data item \n",
        "              val_inputs, val_labels = data\n",
        "              val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "\n",
        "              # Send the data item through the network to get output\n",
        "              val_outputs = net(val_inputs)\n",
        "\n",
        "              # Compute the loss\n",
        "              val_loss = criterion(val_outputs, val_labels)\n",
        "\n",
        "              # Get predictions\n",
        "              _, val_preds = torch.max(val_outputs.data, 1)\n",
        "\n",
        "              # Add to running loss\n",
        "              val_running_loss += val_loss.item()\n",
        "\n",
        "              # Add to running accuracy\n",
        "              val_running_accuracy += (val_preds == val_labels).float().sum()\n",
        "        \n",
        "        # Rescale the training and validation perfomance metrics\n",
        "        running_loss = running_loss/len(train_loader)\n",
        "        running_accuracy = running_accuracy/(len(train_loader)*config['batch_size'])\n",
        "        \n",
        "        # Rescale the validation loss\n",
        "        val_running_loss = val_running_loss/len(test_loader)\n",
        "        val_running_accuracy = val_running_accuracy/(len(test_loader)*config['batch_size'])\n",
        "        \n",
        "        # Make print message format string\n",
        "        msg = \"{}, Epoch:{}, Loss:{}, Accuracy:{},\" \"\\n\"\n",
        "\n",
        "        # Print performance\n",
        "        print(msg.format(\"Training\", epoch, running_loss, running_accuracy))\n",
        "        print(msg.format(\"Validation\", epoch, val_running_loss, val_running_accuracy))\n",
        "        \n",
        "        # Store the end of each epoch as a checkpoint\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "          path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "          torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "        \n",
        "        # Send results back to tune to display in report\n",
        "        tune.report(loss= val_running_loss, accuracy= val_running_accuracy)\n",
        "\n",
        "    # Print message\n",
        "    print('Done training...')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZDKmsdzQ6Ak"
      },
      "source": [
        "def get_tuning_search_space():\n",
        "  '''\n",
        "  Return a grid of configurable parameters for tuning\n",
        "  '''\n",
        "  n_layers = np.random.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "  config = {\n",
        "      \"nb_units\" : [2**np.random.randint(2, 9) for i in range(n_layers)],\n",
        "      \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "      \"momentum\": tune.choice([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]),\n",
        "      \"batch_size\": tune.choice([2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]),\n",
        "      \"weight_decay\": tune.loguniform(1e-4, 1e-1),\n",
        "  }\n",
        "\n",
        "  return(config)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiK7qF9xjCg5"
      },
      "source": [
        "def main( gpus_per_trial = 1, num_samples = 5, max_num_epochs=50):\n",
        "\n",
        "  config = get_tuning_search_space()\n",
        "  data_dir = '/content/drive/MyDrive/data/'\n",
        "\n",
        "  scheduler = ASHAScheduler(metric=\"loss\", mode=\"min\", max_t=max_num_epochs, grace_period=1, reduction_factor=2)\n",
        "  reporter = CLIReporter(metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "\n",
        "  result = tune.run(\n",
        "    partial(train, data_dir=data_dir),\n",
        "    resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "    config=config,\n",
        "    num_samples=num_samples,\n",
        "    scheduler=scheduler,\n",
        "    progress_reporter=reporter,\n",
        "    checkpoint_at_end=False)\n",
        "  \n",
        "  return(result)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1miE-Eclj90x",
        "outputId": "6333af65-d26c-4d4a-e4f4-294d8abfcdfa"
      },
      "source": [
        "result = main()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-01 18:45:03,265\tWARNING experiment.py:274 -- No name detected on trainable. Using DEFAULT.\n",
            "2020-12-01 18:45:03,266\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 2.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.23 GiB heap, 0.0/2.49 GiB objects (0/1.0 accelerator_type:P100)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-01_18-45-03\n",
            "Number of trials: 1/5 (1 RUNNING)\n",
            "+---------------------+----------+-------+--------------+------------+------------+----------------+\n",
            "| Trial name          | status   | loc   |   batch_size |         lr |   momentum |   weight_decay |\n",
            "|---------------------+----------+-------+--------------+------------+------------+----------------|\n",
            "| DEFAULT_52c1e_00000 | RUNNING  |       |           16 | 0.00402083 |        0.4 |     0.00539935 |\n",
            "+---------------------+----------+-------+--------------+------------+------------+----------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m /usr/local/lib/python3.6/dist-packages/ray/workers/default_worker.py:6: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m   \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for DEFAULT_52c1e_00000:\n",
            "  accuracy: tensor(0.4886, device='cuda:0')\n",
            "  date: 2020-12-01_18-45-48\n",
            "  done: false\n",
            "  experiment_id: 72b11e9f6f1140e58399295bce9e81be\n",
            "  experiment_tag: 0_batch_size=16,lr=0.0040208,momentum=0.4,weight_decay=0.0053993\n",
            "  hostname: f4686065c2f2\n",
            "  iterations_since_restore: 1\n",
            "  loss: 1.502837603187561\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2261\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 43.58958196640015\n",
            "  time_this_iter_s: 43.58958196640015\n",
            "  time_total_s: 43.58958196640015\n",
            "  timestamp: 1606848348\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 52c1e_00000\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.502837603187561\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.23 GiB heap, 0.0/2.49 GiB objects (0/1.0 accelerator_type:P100)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-01_18-45-03\n",
            "Number of trials: 2/5 (1 PENDING, 1 RUNNING)\n",
            "+---------------------+----------+-----------------+--------------+------------+------------+----------------+---------+------------+----------------------+\n",
            "| Trial name          | status   | loc             |   batch_size |         lr |   momentum |   weight_decay |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+----------+-----------------+--------------+------------+------------+----------------+---------+------------+----------------------|\n",
            "| DEFAULT_52c1e_00000 | RUNNING  | 172.28.0.2:2261 |           16 | 0.00402083 |        0.4 |     0.00539935 | 1.50284 |     0.4886 |                    1 |\n",
            "| DEFAULT_52c1e_00001 | PENDING  |                 |           32 | 0.00057782 |        0.3 |     0.088358   |         |            |                      |\n",
            "+---------------------+----------+-----------------+--------------+------------+------------+----------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m Training, Epoch:0, Loss:1.692847789065043, Accuracy:0.39606666564941406,\n",
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m Validation, Epoch:0, Loss:1.502837603187561, Accuracy:0.4885999858379364,\n",
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m \n",
            "Result for DEFAULT_52c1e_00000:\n",
            "  accuracy: tensor(0.6233, device='cuda:0')\n",
            "  date: 2020-12-01_18-46-27\n",
            "  done: false\n",
            "  experiment_id: 72b11e9f6f1140e58399295bce9e81be\n",
            "  experiment_tag: 0_batch_size=16,lr=0.0040208,momentum=0.4,weight_decay=0.0053993\n",
            "  hostname: f4686065c2f2\n",
            "  iterations_since_restore: 2\n",
            "  loss: 0.9654188490867615\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2261\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 82.76219201087952\n",
            "  time_this_iter_s: 39.17261004447937\n",
            "  time_total_s: 82.76219201087952\n",
            "  timestamp: 1606848387\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: 52c1e_00000\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.9654188490867615 | Iter 1.000: -1.502837603187561\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.23 GiB heap, 0.0/2.49 GiB objects (0/1.0 accelerator_type:P100)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-01_18-45-03\n",
            "Number of trials: 2/5 (1 PENDING, 1 RUNNING)\n",
            "+---------------------+----------+-----------------+--------------+------------+------------+----------------+----------+------------+----------------------+\n",
            "| Trial name          | status   | loc             |   batch_size |         lr |   momentum |   weight_decay |     loss |   accuracy |   training_iteration |\n",
            "|---------------------+----------+-----------------+--------------+------------+------------+----------------+----------+------------+----------------------|\n",
            "| DEFAULT_52c1e_00000 | RUNNING  | 172.28.0.2:2261 |           16 | 0.00402083 |        0.4 |     0.00539935 | 0.965419 |     0.6233 |                    2 |\n",
            "| DEFAULT_52c1e_00001 | PENDING  |                 |           32 | 0.00057782 |        0.3 |     0.088358   |          |            |                      |\n",
            "+---------------------+----------+-----------------+--------------+------------+------------+----------------+----------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m Training, Epoch:1, Loss:1.221934138337771, Accuracy:0.5632500052452087,\n",
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m Validation, Epoch:1, Loss:0.9654188490867615, Accuracy:0.6232999563217163,\n",
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2261)\u001b[0m Done training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2334)\u001b[0m /usr/local/lib/python3.6/dist-packages/ray/workers/default_worker.py:6: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "\u001b[2m\u001b[36m(pid=2334)\u001b[0m   \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for DEFAULT_52c1e_00001:\n",
            "  accuracy: tensor(0.0998, device='cuda:0')\n",
            "  date: 2020-12-01_18-46-55\n",
            "  done: true\n",
            "  experiment_id: a7223dce15fd49e18790c859d364e493\n",
            "  experiment_tag: 1_batch_size=32,lr=0.00057782,momentum=0.3,weight_decay=0.088358\n",
            "  hostname: f4686065c2f2\n",
            "  iterations_since_restore: 1\n",
            "  loss: 2.3025859902841974\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2334\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 25.69222927093506\n",
            "  time_this_iter_s: 25.69222927093506\n",
            "  time_total_s: 25.69222927093506\n",
            "  timestamp: 1606848415\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 52c1e_00001\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.9654188490867615 | Iter 1.000: -1.902711796735879\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.23 GiB heap, 0.0/2.49 GiB objects (0/1.0 accelerator_type:P100)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-01_18-45-03\n",
            "Number of trials: 3/5 (1 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |          lr |   momentum |   weight_decay |     loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------|\n",
            "| DEFAULT_52c1e_00001 | RUNNING    | 172.28.0.2:2334 |           32 | 0.00057782  |        0.3 |     0.088358   | 2.30259  |  0.0998403 |                    1 |\n",
            "| DEFAULT_52c1e_00002 | PENDING    |                 |           64 | 0.000177508 |        0.2 |     0.00207843 |          |            |                      |\n",
            "| DEFAULT_52c1e_00000 | TERMINATED |                 |           16 | 0.00402083  |        0.4 |     0.00539935 | 0.965419 |  0.6233    |                    2 |\n",
            "+---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2334)\u001b[0m Training, Epoch:0, Loss:2.302816281763713, Accuracy:0.09901666641235352,\n",
            "\u001b[2m\u001b[36m(pid=2334)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2334)\u001b[0m Validation, Epoch:0, Loss:2.3025859902841974, Accuracy:0.09984025359153748,\n",
            "\u001b[2m\u001b[36m(pid=2334)\u001b[0m \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-01 18:46:56,865\tWARNING worker.py:1091 -- The actor or task with ID ffffffffffffffffb5dec61801000000 is pending and cannot currently be scheduled. It requires {CPU: 2.000000}, {GPU: 1.000000} for execution and {CPU: 2.000000}, {GPU: 1.000000} for placement, but this node only has remaining {GPU: 1.000000}, {node:172.28.0.2: 1.000000}, {accelerator_type:P100: 1.000000}, {CPU: 2.000000}, {memory: 7.226562 GiB}, {object_store_memory: 2.490234 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m /usr/local/lib/python3.6/dist-packages/ray/workers/default_worker.py:6: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m   \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for DEFAULT_52c1e_00002:\n",
            "  accuracy: tensor(0.4758, device='cuda:0')\n",
            "  date: 2020-12-01_18-47-13\n",
            "  done: false\n",
            "  experiment_id: 649514b4cb7b463d83a97adba80c0db7\n",
            "  experiment_tag: 2_batch_size=64,lr=0.00017751,momentum=0.2,weight_decay=0.0020784\n",
            "  hostname: f4686065c2f2\n",
            "  iterations_since_restore: 1\n",
            "  loss: 1.339751679806193\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2380\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 15.146131038665771\n",
            "  time_this_iter_s: 15.146131038665771\n",
            "  time_total_s: 15.146131038665771\n",
            "  timestamp: 1606848433\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 52c1e_00002\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.9654188490867615 | Iter 1.000: -1.502837603187561\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.23 GiB heap, 0.0/2.49 GiB objects (0/1.0 accelerator_type:P100)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-01_18-45-03\n",
            "Number of trials: 4/5 (1 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |          lr |   momentum |   weight_decay |     loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------|\n",
            "| DEFAULT_52c1e_00002 | RUNNING    | 172.28.0.2:2380 |           64 | 0.000177508 |        0.2 |    0.00207843  | 1.33975  |  0.475816  |                    1 |\n",
            "| DEFAULT_52c1e_00003 | PENDING    |                 |           16 | 0.0151858   |        0.8 |    0.000421443 |          |            |                      |\n",
            "| DEFAULT_52c1e_00000 | TERMINATED |                 |           16 | 0.00402083  |        0.4 |    0.00539935  | 0.965419 |  0.6233    |                    2 |\n",
            "| DEFAULT_52c1e_00001 | TERMINATED |                 |           32 | 0.00057782  |        0.3 |    0.088358    | 2.30259  |  0.0998403 |                    1 |\n",
            "+---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m Training, Epoch:0, Loss:1.6435942320045887, Accuracy:0.42385727167129517,\n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m Validation, Epoch:0, Loss:1.339751679806193, Accuracy:0.47581610083580017,\n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m \n",
            "Result for DEFAULT_52c1e_00002:\n",
            "  accuracy: tensor(0.7331, device='cuda:0')\n",
            "  date: 2020-12-01_18-47-24\n",
            "  done: false\n",
            "  experiment_id: 649514b4cb7b463d83a97adba80c0db7\n",
            "  experiment_tag: 2_batch_size=64,lr=0.00017751,momentum=0.2,weight_decay=0.0020784\n",
            "  hostname: f4686065c2f2\n",
            "  iterations_since_restore: 2\n",
            "  loss: 0.7165822800557324\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2380\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 25.767371892929077\n",
            "  time_this_iter_s: 10.621240854263306\n",
            "  time_total_s: 25.767371892929077\n",
            "  timestamp: 1606848444\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: 52c1e_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m Training, Epoch:1, Loss:0.9831656277942251, Accuracy:0.6258162260055542,\n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m Validation, Epoch:1, Loss:0.7165822800557324, Accuracy:0.7330812215805054,\n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m \n",
            "== Status ==\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.841000564571247 | Iter 1.000: -1.502837603187561\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.23 GiB heap, 0.0/2.49 GiB objects (0/1.0 accelerator_type:P100)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-01_18-45-03\n",
            "Number of trials: 4/5 (1 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |          lr |   momentum |   weight_decay |     loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------|\n",
            "| DEFAULT_52c1e_00002 | RUNNING    | 172.28.0.2:2380 |           64 | 0.000177508 |        0.2 |    0.00207843  | 0.716582 |  0.733081  |                    2 |\n",
            "| DEFAULT_52c1e_00003 | PENDING    |                 |           16 | 0.0151858   |        0.8 |    0.000421443 |          |            |                      |\n",
            "| DEFAULT_52c1e_00000 | TERMINATED |                 |           16 | 0.00402083  |        0.4 |    0.00539935  | 0.965419 |  0.6233    |                    2 |\n",
            "| DEFAULT_52c1e_00001 | TERMINATED |                 |           32 | 0.00057782  |        0.3 |    0.088358    | 2.30259  |  0.0998403 |                    1 |\n",
            "+---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2380)\u001b[0m Done training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2442)\u001b[0m /usr/local/lib/python3.6/dist-packages/ray/workers/default_worker.py:6: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "\u001b[2m\u001b[36m(pid=2442)\u001b[0m   \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for DEFAULT_52c1e_00003:\n",
            "  accuracy: tensor(0.1000, device='cuda:0')\n",
            "  date: 2020-12-01_18-48-12\n",
            "  done: true\n",
            "  experiment_id: 49ea565943a04078b7913b28c4b91546\n",
            "  experiment_tag: 3_batch_size=16,lr=0.015186,momentum=0.8,weight_decay=0.00042144\n",
            "  hostname: f4686065c2f2\n",
            "  iterations_since_restore: 1\n",
            "  loss: 2.3025853633880615\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2442\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 46.69238042831421\n",
            "  time_this_iter_s: 46.69238042831421\n",
            "  time_total_s: 46.69238042831421\n",
            "  timestamp: 1606848492\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 52c1e_00003\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.841000564571247 | Iter 1.000: -1.9027114832878111\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.23 GiB heap, 0.0/2.49 GiB objects (0/1.0 accelerator_type:P100)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-01_18-45-03\n",
            "Number of trials: 5/5 (1 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |          lr |   momentum |   weight_decay |     loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------|\n",
            "| DEFAULT_52c1e_00003 | RUNNING    | 172.28.0.2:2442 |           16 | 0.0151858   |        0.8 |    0.000421443 | 2.30259  |  0.1       |                    1 |\n",
            "| DEFAULT_52c1e_00004 | PENDING    |                 |           32 | 0.0152595   |        0.2 |    0.0289667   |          |            |                      |\n",
            "| DEFAULT_52c1e_00000 | TERMINATED |                 |           16 | 0.00402083  |        0.4 |    0.00539935  | 0.965419 |  0.6233    |                    2 |\n",
            "| DEFAULT_52c1e_00001 | TERMINATED |                 |           32 | 0.00057782  |        0.3 |    0.088358    | 2.30259  |  0.0998403 |                    1 |\n",
            "| DEFAULT_52c1e_00002 | TERMINATED |                 |           64 | 0.000177508 |        0.2 |    0.00207843  | 0.716582 |  0.733081  |                    2 |\n",
            "+---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2442)\u001b[0m Training, Epoch:0, Loss:2.313889375750224, Accuracy:0.0997999981045723,\n",
            "\u001b[2m\u001b[36m(pid=2442)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2442)\u001b[0m Validation, Epoch:0, Loss:2.3025853633880615, Accuracy:0.09999999403953552,\n",
            "\u001b[2m\u001b[36m(pid=2442)\u001b[0m \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2494)\u001b[0m /usr/local/lib/python3.6/dist-packages/ray/workers/default_worker.py:6: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "\u001b[2m\u001b[36m(pid=2494)\u001b[0m   \n",
            "2020-12-01 18:48:40,569\tINFO tune.py:439 -- Total run time: 217.31 seconds (217.25 seconds for the tuning loop).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for DEFAULT_52c1e_00004:\n",
            "  accuracy: tensor(0.0998, device='cuda:0')\n",
            "  date: 2020-12-01_18-48-40\n",
            "  done: true\n",
            "  experiment_id: 128b83f096cf4c7a9afb1f8bbe442596\n",
            "  experiment_tag: 4_batch_size=32,lr=0.01526,momentum=0.2,weight_decay=0.028967\n",
            "  hostname: f4686065c2f2\n",
            "  iterations_since_restore: 1\n",
            "  loss: 2.3025848880743447\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2494\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 25.229898691177368\n",
            "  time_this_iter_s: 25.229898691177368\n",
            "  time_total_s: 25.229898691177368\n",
            "  timestamp: 1606848520\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 52c1e_00004\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.841000564571247 | Iter 1.000: -2.3025848880743447\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.23 GiB heap, 0.0/2.49 GiB objects (0/1.0 accelerator_type:P100)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-01_18-45-03\n",
            "Number of trials: 5/5 (1 RUNNING, 4 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |          lr |   momentum |   weight_decay |     loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------|\n",
            "| DEFAULT_52c1e_00004 | RUNNING    | 172.28.0.2:2494 |           32 | 0.0152595   |        0.2 |    0.0289667   | 2.30258  |  0.0998403 |                    1 |\n",
            "| DEFAULT_52c1e_00000 | TERMINATED |                 |           16 | 0.00402083  |        0.4 |    0.00539935  | 0.965419 |  0.6233    |                    2 |\n",
            "| DEFAULT_52c1e_00001 | TERMINATED |                 |           32 | 0.00057782  |        0.3 |    0.088358    | 2.30259  |  0.0998403 |                    1 |\n",
            "| DEFAULT_52c1e_00002 | TERMINATED |                 |           64 | 0.000177508 |        0.2 |    0.00207843  | 0.716582 |  0.733081  |                    2 |\n",
            "| DEFAULT_52c1e_00003 | TERMINATED |                 |           16 | 0.0151858   |        0.8 |    0.000421443 | 2.30259  |  0.1       |                    1 |\n",
            "+---------------------+------------+-----------------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2494)\u001b[0m Training, Epoch:0, Loss:2.303073534647624, Accuracy:0.10088333487510681,\n",
            "\u001b[2m\u001b[36m(pid=2494)\u001b[0m \n",
            "\u001b[2m\u001b[36m(pid=2494)\u001b[0m Validation, Epoch:0, Loss:2.3025848880743447, Accuracy:0.09984025359153748,\n",
            "\u001b[2m\u001b[36m(pid=2494)\u001b[0m \n",
            "== Status ==\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -0.841000564571247 | Iter 1.000: -2.3025848880743447\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.23 GiB heap, 0.0/2.49 GiB objects (0/1.0 accelerator_type:P100)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-01_18-45-03\n",
            "Number of trials: 5/5 (5 TERMINATED)\n",
            "+---------------------+------------+-------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "| Trial name          | status     | loc   |   batch_size |          lr |   momentum |   weight_decay |     loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-------+--------------+-------------+------------+----------------+----------+------------+----------------------|\n",
            "| DEFAULT_52c1e_00000 | TERMINATED |       |           16 | 0.00402083  |        0.4 |    0.00539935  | 0.965419 |  0.6233    |                    2 |\n",
            "| DEFAULT_52c1e_00001 | TERMINATED |       |           32 | 0.00057782  |        0.3 |    0.088358    | 2.30259  |  0.0998403 |                    1 |\n",
            "| DEFAULT_52c1e_00002 | TERMINATED |       |           64 | 0.000177508 |        0.2 |    0.00207843  | 0.716582 |  0.733081  |                    2 |\n",
            "| DEFAULT_52c1e_00003 | TERMINATED |       |           16 | 0.0151858   |        0.8 |    0.000421443 | 2.30259  |  0.1       |                    1 |\n",
            "| DEFAULT_52c1e_00004 | TERMINATED |       |           32 | 0.0152595   |        0.2 |    0.0289667   | 2.30258  |  0.0998403 |                    1 |\n",
            "+---------------------+------------+-------+--------------+-------------+------------+----------------+----------+------------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}