{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "mlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOnHtZg1taNw"
      },
      "source": [
        "# To do\n",
        "# Test out variable layers\n",
        "# Add regularization\n",
        "# Add hyper-parameter tuning\n",
        "# Check everything\n",
        "# Run"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WDdC5E0llYu",
        "outputId": "dab63dc4-76bf-4339-a7bd-ec5ddd4ca6b9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XQHea_TnP4S",
        "outputId": "10184897-1050-4505-99ba-ffab5ac45c0a"
      },
      "source": [
        "# Install idx2numpy package for extracting data\n",
        "!pip install idx2numpy"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: idx2numpy in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9CsmgA1jCg4"
      },
      "source": [
        "# Import packages\n",
        "import gzip\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np \n",
        "\n",
        "import idx2numpy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iQ0XWiQjyQs",
        "outputId": "8f5eb030-7c21-4eb1-bf32-f013cbd1d1cc"
      },
      "source": [
        "# Mount Google drive to access data from Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmfcKJJCjCg5"
      },
      "source": [
        "def load_one_dataset(path):\n",
        "    '''\n",
        "    Convenience function to load a single dataset\n",
        "    '''\n",
        "    f = gzip.open(path, 'rb')\n",
        "    data = torch.from_numpy(idx2numpy.convert_from_file(f))\n",
        "    f.close()\n",
        "    \n",
        "    return(data)\n",
        "\n",
        "\n",
        "def load_all_datasets(train_imgs, train_labs, test_imgs, test_labs, batch_size):\n",
        "    '''\n",
        "    Load training as well as test images here\n",
        "    '''\n",
        "    \n",
        "    train_images = load_one_dataset(train_imgs).type(torch.float32)\n",
        "    train_labels = load_one_dataset(train_labs).type(torch.long)\n",
        "    train = list(zip(train_images, train_labels))\n",
        "    \n",
        "    test_images = load_one_dataset(test_imgs).type(torch.float32)\n",
        "    test_labels = load_one_dataset(test_labs).type(torch.long)\n",
        "    test = list(zip(test_images, test_labels))\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    return(train_loader, test_loader)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxKAhnC9rieJ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  \n",
        "  def __init__(self, nb_units, input_dim, output_dim):\n",
        "    '''\n",
        "    Declare the network architecture here\n",
        "    '''\n",
        "    super(Net, self).__init__()\n",
        "    \n",
        "    # Initialize a list to store layers\n",
        "    fc = []\n",
        "\n",
        "    # Add input and output dimensions to layer list\n",
        "    self.nb_units = [input_dim] + nb_units + [output_dim]\n",
        "\n",
        "    # Now compute the total no. of layers\n",
        "    self.nb_layers = len(self.nb_units)\n",
        "\n",
        "    # Now append the hidden layers\n",
        "    for i in range(1, self.nb_layers):\n",
        "      fc.append(nn.Linear(self.nb_units[i-1], self.nb_units[i]))\n",
        "    \n",
        "    # Wrap this in a module list \n",
        "    self.fc = nn.ModuleList(fc)\n",
        "    \n",
        "  \n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Send input forward through \n",
        "    the network\n",
        "    '''\n",
        "    # Reshape 28X28 images to be 784 X 784\n",
        "    x = x.view(-1, 28*28)\n",
        "\n",
        "    # Send example through network\n",
        "    for layer in self.fc: x = F.relu(layer(x))\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGe3H8TIjCg5"
      },
      "source": [
        "def train(nb_units=[256, 128, 64, 32, 16], input_dim=784, output_dim = 10, \n",
        "          epochs=2, lr=0.001, momentum=0.9, batch_size=256):\n",
        "    '''\n",
        "    This is the main training loop\n",
        "    '''\n",
        "    \n",
        "    # Set paths to datasets\n",
        "    paths = {\n",
        "        \n",
        "        'train_imgs': '/content/gdrive/MyDrive/data/train-images-idx3-ubyte.gz',\n",
        "        'train_labs': '/content/gdrive/MyDrive/data/train-labels-idx1-ubyte.gz',\n",
        "        'test_imgs': '/content/gdrive/MyDrive/data/t10k-images-idx3-ubyte.gz',\n",
        "        'test_labs': '/content/gdrive/MyDrive/data/t10k-labels-idx1-ubyte.gz'\n",
        "    }\n",
        "    \n",
        "    # Load datasets\n",
        "    train_loader, test_loader = load_all_datasets(**paths, batch_size = batch_size)\n",
        "    \n",
        "    # Set parameters\n",
        "    net = Net(nb_units, input_dim, output_dim)\n",
        "    \n",
        "    # We use the cross-entropy loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # We use mini-batch stochastic gradient descent with momentum\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "    \n",
        "    # Loop over the dataset multiple times\n",
        "    for epoch in range(epochs):  \n",
        "        \n",
        "        # Initialize running loss\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        # Iterate through data now\n",
        "        for i, data in enumerate(train_loader):\n",
        "            \n",
        "            # Get the inputs: data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            \n",
        "            # Optimize\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        # Initialize the validation running loss\n",
        "        val_running_loss = 0.0\n",
        "        \n",
        "        # Loop through the validation data\n",
        "        for j, data in enumerate(test_loader):\n",
        "          \n",
        "          # No need to calculate gradients for validation set\n",
        "          with torch.no_grad():\n",
        "\n",
        "              # Get the data item \n",
        "              val_inputs, val_labels = data\n",
        "\n",
        "              # Send the data item through the network to get output\n",
        "              val_outputs = net(val_inputs)\n",
        "\n",
        "              # Compute the loss\n",
        "              val_loss = criterion(val_outputs, val_labels)\n",
        "\n",
        "              # Add to the running validation loss\n",
        "              val_running_loss += val_loss.item()\n",
        "            \n",
        "        # Print train loss\n",
        "        print(\"The train loss on epoch {} is {}...\".format(epoch, running_loss))\n",
        "        \n",
        "        # Print validation loss\n",
        "        print(\"The validation loss on epoch {} is {}...\".format(epoch, val_running_loss))\n",
        "    \n",
        "    # Print message\n",
        "    print('Done training...')"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiK7qF9xjCg5",
        "outputId": "5a20a28e-9a02-4baa-881a-72d10ca5e4ac"
      },
      "source": [
        "train(epochs=100)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train loss on epoch 0 is 303.3239602446556...\n",
            "The validation loss on epoch 0 is 38.306807816028595...\n",
            "The train loss on epoch 1 is 212.50751334428787...\n",
            "The validation loss on epoch 1 is 35.803361654281616...\n",
            "The train loss on epoch 2 is 199.51816880702972...\n",
            "The validation loss on epoch 2 is 34.94192507863045...\n",
            "The train loss on epoch 3 is 192.23022776842117...\n",
            "The validation loss on epoch 3 is 33.41094583272934...\n",
            "The train loss on epoch 4 is 187.6843690276146...\n",
            "The validation loss on epoch 4 is 32.732335805892944...\n",
            "The train loss on epoch 5 is 184.3571653366089...\n",
            "The validation loss on epoch 5 is 33.24167376756668...\n",
            "The train loss on epoch 6 is 181.8697247505188...\n",
            "The validation loss on epoch 6 is 31.863867074251175...\n",
            "The train loss on epoch 7 is 177.92119485139847...\n",
            "The validation loss on epoch 7 is 32.69438564777374...\n",
            "The train loss on epoch 8 is 175.32000172138214...\n",
            "The validation loss on epoch 8 is 31.670001447200775...\n",
            "The train loss on epoch 9 is 173.8351308107376...\n",
            "The validation loss on epoch 9 is 31.689741522073746...\n",
            "The train loss on epoch 10 is 171.9643658399582...\n",
            "The validation loss on epoch 10 is 31.684565722942352...\n",
            "The train loss on epoch 11 is 170.90322893857956...\n",
            "The validation loss on epoch 11 is 31.602120250463486...\n",
            "The train loss on epoch 12 is 168.29139477014542...\n",
            "The validation loss on epoch 12 is 30.938969373703003...\n",
            "The train loss on epoch 13 is 166.99025267362595...\n",
            "The validation loss on epoch 13 is 31.50968873500824...\n",
            "The train loss on epoch 14 is 165.78775185346603...\n",
            "The validation loss on epoch 14 is 31.540278404951096...\n",
            "The train loss on epoch 15 is 164.20634645223618...\n",
            "The validation loss on epoch 15 is 30.974197804927826...\n",
            "The train loss on epoch 16 is 163.84887367486954...\n",
            "The validation loss on epoch 16 is 31.341447591781616...\n",
            "The train loss on epoch 17 is 161.64421519637108...\n",
            "The validation loss on epoch 17 is 30.76256638765335...\n",
            "The train loss on epoch 18 is 160.0426069498062...\n",
            "The validation loss on epoch 18 is 31.229724884033203...\n",
            "The train loss on epoch 19 is 159.19590431451797...\n",
            "The validation loss on epoch 19 is 31.801205337047577...\n",
            "The train loss on epoch 20 is 158.61909571290016...\n",
            "The validation loss on epoch 20 is 30.654591262340546...\n",
            "The train loss on epoch 21 is 158.01409935951233...\n",
            "The validation loss on epoch 21 is 32.279796063899994...\n",
            "The train loss on epoch 22 is 157.18308082222939...\n",
            "The validation loss on epoch 22 is 30.933082282543182...\n",
            "The train loss on epoch 23 is 155.26392018795013...\n",
            "The validation loss on epoch 23 is 30.80785644054413...\n",
            "The train loss on epoch 24 is 154.77316689491272...\n",
            "The validation loss on epoch 24 is 30.744869470596313...\n",
            "The train loss on epoch 25 is 154.38845771551132...\n",
            "The validation loss on epoch 25 is 30.874238967895508...\n",
            "The train loss on epoch 26 is 152.9503874182701...\n",
            "The validation loss on epoch 26 is 31.888651490211487...\n",
            "The train loss on epoch 27 is 152.70272207260132...\n",
            "The validation loss on epoch 27 is 31.15004074573517...\n",
            "The train loss on epoch 28 is 152.03297275304794...\n",
            "The validation loss on epoch 28 is 30.590223252773285...\n",
            "The train loss on epoch 29 is 150.21094825863838...\n",
            "The validation loss on epoch 29 is 30.569462418556213...\n",
            "The train loss on epoch 30 is 150.39710381627083...\n",
            "The validation loss on epoch 30 is 31.743485867977142...\n",
            "The train loss on epoch 31 is 150.0818192064762...\n",
            "The validation loss on epoch 31 is 31.41623955965042...\n",
            "The train loss on epoch 32 is 148.05416625738144...\n",
            "The validation loss on epoch 32 is 31.236458897590637...\n",
            "The train loss on epoch 33 is 147.42160633206367...\n",
            "The validation loss on epoch 33 is 30.584210991859436...\n",
            "The train loss on epoch 34 is 147.16120982170105...\n",
            "The validation loss on epoch 34 is 31.464917421340942...\n",
            "The train loss on epoch 35 is 146.30699902772903...\n",
            "The validation loss on epoch 35 is 30.88875460624695...\n",
            "The train loss on epoch 36 is 145.0486200451851...\n",
            "The validation loss on epoch 36 is 31.521698474884033...\n",
            "The train loss on epoch 37 is 145.96408420801163...\n",
            "The validation loss on epoch 37 is 31.059873461723328...\n",
            "The train loss on epoch 38 is 144.50922653079033...\n",
            "The validation loss on epoch 38 is 31.6719092130661...\n",
            "The train loss on epoch 39 is 143.95871856808662...\n",
            "The validation loss on epoch 39 is 32.33312112092972...\n",
            "The train loss on epoch 40 is 143.70261564850807...\n",
            "The validation loss on epoch 40 is 32.39210879802704...\n",
            "The train loss on epoch 41 is 142.45370829105377...\n",
            "The validation loss on epoch 41 is 31.212599515914917...\n",
            "The train loss on epoch 42 is 141.68014806509018...\n",
            "The validation loss on epoch 42 is 31.914210379123688...\n",
            "The train loss on epoch 43 is 141.22114312648773...\n",
            "The validation loss on epoch 43 is 31.4487264752388...\n",
            "The train loss on epoch 44 is 140.43959826231003...\n",
            "The validation loss on epoch 44 is 32.11614143848419...\n",
            "The train loss on epoch 45 is 140.32575258612633...\n",
            "The validation loss on epoch 45 is 32.60118770599365...\n",
            "The train loss on epoch 46 is 140.0915255844593...\n",
            "The validation loss on epoch 46 is 32.98575305938721...\n",
            "The train loss on epoch 47 is 140.04814797639847...\n",
            "The validation loss on epoch 47 is 31.462730765342712...\n",
            "The train loss on epoch 48 is 140.1365012228489...\n",
            "The validation loss on epoch 48 is 31.22751921415329...\n",
            "The train loss on epoch 49 is 139.04574924707413...\n",
            "The validation loss on epoch 49 is 31.605462074279785...\n",
            "The train loss on epoch 50 is 137.9729574918747...\n",
            "The validation loss on epoch 50 is 31.55014705657959...\n",
            "The train loss on epoch 51 is 136.69906395673752...\n",
            "The validation loss on epoch 51 is 32.548058211803436...\n",
            "The train loss on epoch 52 is 136.82668808102608...\n",
            "The validation loss on epoch 52 is 32.26776349544525...\n",
            "The train loss on epoch 53 is 136.13182213902473...\n",
            "The validation loss on epoch 53 is 32.264399111270905...\n",
            "The train loss on epoch 54 is 136.11713683605194...\n",
            "The validation loss on epoch 54 is 33.18514966964722...\n",
            "The train loss on epoch 55 is 135.8246632516384...\n",
            "The validation loss on epoch 55 is 32.428567588329315...\n",
            "The train loss on epoch 56 is 134.69766426086426...\n",
            "The validation loss on epoch 56 is 33.19775599241257...\n",
            "The train loss on epoch 57 is 135.87497156858444...\n",
            "The validation loss on epoch 57 is 32.792479157447815...\n",
            "The train loss on epoch 58 is 137.05073961615562...\n",
            "The validation loss on epoch 58 is 31.72451138496399...\n",
            "The train loss on epoch 59 is 135.2647344470024...\n",
            "The validation loss on epoch 59 is 32.77875703573227...\n",
            "The train loss on epoch 60 is 133.79121840000153...\n",
            "The validation loss on epoch 60 is 33.209366261959076...\n",
            "The train loss on epoch 61 is 133.18942394852638...\n",
            "The validation loss on epoch 61 is 32.32273858785629...\n",
            "The train loss on epoch 62 is 133.2976227402687...\n",
            "The validation loss on epoch 62 is 33.79458385705948...\n",
            "The train loss on epoch 63 is 132.9889912903309...\n",
            "The validation loss on epoch 63 is 32.46829569339752...\n",
            "The train loss on epoch 64 is 132.69478657841682...\n",
            "The validation loss on epoch 64 is 32.6640904545784...\n",
            "The train loss on epoch 65 is 133.10685569047928...\n",
            "The validation loss on epoch 65 is 33.112687051296234...\n",
            "The train loss on epoch 66 is 131.1806848347187...\n",
            "The validation loss on epoch 66 is 33.70574951171875...\n",
            "The train loss on epoch 67 is 131.40154126286507...\n",
            "The validation loss on epoch 67 is 32.78472262620926...\n",
            "The train loss on epoch 68 is 131.8724563419819...\n",
            "The validation loss on epoch 68 is 33.59376418590546...\n",
            "The train loss on epoch 69 is 130.8317157626152...\n",
            "The validation loss on epoch 69 is 34.21137285232544...\n",
            "The train loss on epoch 70 is 130.22962349653244...\n",
            "The validation loss on epoch 70 is 33.03593647480011...\n",
            "The train loss on epoch 71 is 130.99260410666466...\n",
            "The validation loss on epoch 71 is 33.63369411230087...\n",
            "The train loss on epoch 72 is 130.16340002417564...\n",
            "The validation loss on epoch 72 is 33.23450458049774...\n",
            "The train loss on epoch 73 is 130.30214130878448...\n",
            "The validation loss on epoch 73 is 33.4952991604805...\n",
            "The train loss on epoch 74 is 129.69024139642715...\n",
            "The validation loss on epoch 74 is 32.73378413915634...\n",
            "The train loss on epoch 75 is 129.44084185361862...\n",
            "The validation loss on epoch 75 is 33.55363863706589...\n",
            "The train loss on epoch 76 is 128.34240812063217...\n",
            "The validation loss on epoch 76 is 37.43546175956726...\n",
            "The train loss on epoch 77 is 128.69088384509087...\n",
            "The validation loss on epoch 77 is 33.874571621418...\n",
            "The train loss on epoch 78 is 128.05384814739227...\n",
            "The validation loss on epoch 78 is 34.107562363147736...\n",
            "The train loss on epoch 79 is 128.4810599386692...\n",
            "The validation loss on epoch 79 is 33.881147384643555...\n",
            "The train loss on epoch 80 is 128.05582058429718...\n",
            "The validation loss on epoch 80 is 33.31472906470299...\n",
            "The train loss on epoch 81 is 127.55049493908882...\n",
            "The validation loss on epoch 81 is 35.237607181072235...\n",
            "The train loss on epoch 82 is 126.16930881142616...\n",
            "The validation loss on epoch 82 is 33.90619158744812...\n",
            "The train loss on epoch 83 is 128.75582709908485...\n",
            "The validation loss on epoch 83 is 34.88857787847519...\n",
            "The train loss on epoch 84 is 126.95138642191887...\n",
            "The validation loss on epoch 84 is 34.57010567188263...\n",
            "The train loss on epoch 85 is 126.11062636971474...\n",
            "The validation loss on epoch 85 is 34.13281601667404...\n",
            "The train loss on epoch 86 is 125.56981620192528...\n",
            "The validation loss on epoch 86 is 34.91669690608978...\n",
            "The train loss on epoch 87 is 125.34765848517418...\n",
            "The validation loss on epoch 87 is 34.75999182462692...\n",
            "The train loss on epoch 88 is 125.77475497126579...\n",
            "The validation loss on epoch 88 is 33.84959000349045...\n",
            "The train loss on epoch 89 is 125.79172214865685...\n",
            "The validation loss on epoch 89 is 35.23422026634216...\n",
            "The train loss on epoch 90 is 128.7598641514778...\n",
            "The validation loss on epoch 90 is 35.03063905239105...\n",
            "The train loss on epoch 91 is 124.96278402209282...\n",
            "The validation loss on epoch 91 is 34.74899208545685...\n",
            "The train loss on epoch 92 is 125.660270601511...\n",
            "The validation loss on epoch 92 is 34.37043219804764...\n",
            "The train loss on epoch 93 is 124.45961141586304...\n",
            "The validation loss on epoch 93 is 34.70095580816269...\n",
            "The train loss on epoch 94 is 123.7522836625576...\n",
            "The validation loss on epoch 94 is 37.123861372470856...\n",
            "The train loss on epoch 95 is 123.72849661111832...\n",
            "The validation loss on epoch 95 is 34.038600236177444...\n",
            "The train loss on epoch 96 is 127.62545132637024...\n",
            "The validation loss on epoch 96 is 36.051791071891785...\n",
            "The train loss on epoch 97 is 123.63289654254913...\n",
            "The validation loss on epoch 97 is 36.024028182029724...\n",
            "The train loss on epoch 98 is 122.33778455853462...\n",
            "The validation loss on epoch 98 is 34.10434079170227...\n",
            "The train loss on epoch 99 is 124.17285814881325...\n",
            "The validation loss on epoch 99 is 36.71791362762451...\n",
            "Done training...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}