{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "3_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WDdC5E0llYu",
        "outputId": "bad35fa3-2610-4475-d393-1064b97baa88"
      },
      "source": [
        "# Mount Google Drive so we can access data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XQHea_TnP4S",
        "outputId": "cca69cfd-f988-4a58-c142-3e08d5160dab"
      },
      "source": [
        "# Install idx2numpy package for extracting data\n",
        "!pip install idx2numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: idx2numpy in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from idx2numpy) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9CsmgA1jCg4"
      },
      "source": [
        "# Import packages\n",
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "import idx2numpy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as transforms\n",
        "import torchvision.transforms as trans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import OrderedDict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmfcKJJCjCg5"
      },
      "source": [
        "def load_one_dataset(path):\n",
        "    '''\n",
        "    Convenience function to load a single dataset\n",
        "    '''\n",
        "    f = gzip.open(path, 'rb')\n",
        "    data = torch.from_numpy(idx2numpy.convert_from_file(f).astype('float64'))\n",
        "    f.close()\n",
        "    \n",
        "    return(data)\n",
        "\n",
        "\n",
        "def load_all_datasets(train_imgs, train_labs, test_imgs, test_labs, batch_size):\n",
        "    '''\n",
        "    Load training as well as test images here\n",
        "    '''\n",
        "    train_images = load_one_dataset(train_imgs).type(torch.float32)/255.0\n",
        "    train_labels = load_one_dataset(train_labs).type(torch.long)\n",
        "    train = list(zip(train_images, train_labels))\n",
        "\n",
        "    test_images = load_one_dataset(test_imgs).type(torch.float32)/255.0\n",
        "    test_labels = load_one_dataset(test_labs).type(torch.long)\n",
        "    test = list(zip(test_images, test_labels))\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    return(train_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY9lnBvi8W8U"
      },
      "source": [
        "def add_noise(img, quadrants):\n",
        "  '''\n",
        "  Randomly remove 1 or 2 quadrants\n",
        "  from the input image.\n",
        "  '''\n",
        "  # Get the number of quadrants to erase\n",
        "  n_quads_to_erase = np.random.choice([1, 2])\n",
        "\n",
        "  # Get which quadrants to erase\n",
        "  quads_to_erase = np.random.choice([1, 2, 3, 4], size = n_quads_to_erase)\n",
        "\n",
        "  # Create a copy of the image\n",
        "  noisy_img = img.clone()\n",
        "\n",
        "  # Now erase the quadrants\n",
        "  for quad in quads_to_erase:\n",
        "    noisy_img = transforms.erase(noisy_img, *quadrants[quad])\n",
        "  \n",
        "  # Return statement\n",
        "  return(noisy_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMPdTKZsp1Da"
      },
      "source": [
        "def plot_image(img):\n",
        "  '''\n",
        "  Take an image stored as a Torch\n",
        "  tensor and display it in the notebook\n",
        "  '''\n",
        "  # Display the img\n",
        "  plt.imshow(img.numpy(), cmap= 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WomUkeFVrI7-"
      },
      "source": [
        "def test_noise():\n",
        "  '''\n",
        "  Test the noise function\n",
        "  '''\n",
        "  # Just for testing out noise function\n",
        "  data_dir = '/content/drive/MyDrive/data'\n",
        "\n",
        "  # Set paths\n",
        "  paths = {\n",
        "        'train_imgs': os.path.join(data_dir, 'train-images-idx3-ubyte.gz'),\n",
        "        'train_labs': os.path.join(data_dir, 'train-labels-idx1-ubyte.gz'),\n",
        "        'test_imgs': os.path.join(data_dir,'t10k-images-idx3-ubyte.gz'),\n",
        "        'test_labs': os.path.join(data_dir,'t10k-labels-idx1-ubyte.gz')\n",
        "  }\n",
        "\n",
        "  # Load datasets\n",
        "  train_loader, test_loader = load_all_datasets(**paths, batch_size = 32)\n",
        "\n",
        "  # Get the next batch from the train loader\n",
        "  images, labels = iter(train_loader).next()\n",
        "\n",
        "  # Store the quadrant definitions: move this into training loop later\n",
        "  quadrants = {\n",
        "      \n",
        "      1: [0, 0, 14, 14, 255], \n",
        "      2: [0, 14, 14, 14, 255],\n",
        "      3: [14, 0, 14, 14, 255],\n",
        "      4: [14, 14, 14, 14, 255],\n",
        "  }\n",
        "\n",
        "  # Take the first image in the batch\n",
        "  img = images[0]\n",
        "\n",
        "  # Get noisy image\n",
        "  noisy_img = add_noise(img, quadrants)\n",
        "\n",
        "  # Return statement\n",
        "  return(img, noisy_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6rjOJekfxx9"
      },
      "source": [
        "class DenoisingEncoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, encoder_units, decoder_units, input_dim, output_dim):\n",
        "    \n",
        "    # Conventional super-class declaration\n",
        "    super(DenoisingEncoder,self).__init__()\n",
        "\n",
        "    # Initialize lists to store layers\n",
        "    encoder = []\n",
        "    decoder = []\n",
        "\n",
        "    # Add input and output dimensions to layer list for encoder\n",
        "    self.encoder_units = [input_dim] + encoder_units\n",
        "    self.decoder_units = [encoder_units[-1]] + decoder_units + [output_dim]\n",
        "\n",
        "    # Compute the total no. of layers for the encoder/decoder\n",
        "    self.encoder_layers = len(self.encoder_units)\n",
        "    self.decoder_layers = len(self.decoder_units)\n",
        "\n",
        "    # Append the hidden layers for the encoder\n",
        "    for i in range(1, self.encoder_layers):\n",
        "      \n",
        "      # Add linear layer\n",
        "      layer = ('Linear{}'.format(i), nn.Linear(self.encoder_units[i-1], self.encoder_units[i]))\n",
        "      activation = ('RELU{}'.format(i), nn.ReLU(True))\n",
        "      \n",
        "      # Append\n",
        "      encoder.append(layer)\n",
        "      encoder.append(activation)\n",
        "    \n",
        "    # Append the hidden layers for the decoder\n",
        "    for i in range(1, self.decoder_layers - 1):\n",
        "      \n",
        "      # Add the layers\n",
        "      layer = ('Linear{}'.format(i), nn.Linear(self.decoder_units[i-1], self.decoder_units[i]))\n",
        "      activation = ('RELU{}'.format(i), nn.ReLU(True))\n",
        "      \n",
        "      # Append to the lists\n",
        "      decoder.append(layer)\n",
        "      decoder.append(activation)\n",
        "\n",
        "    # Create final output layer\n",
        "    i = self.decoder_layers - 1\n",
        "    layer = ('Linear{}'.format(i), nn.Linear(self.decoder_units[i-1], self.decoder_units[i]))\n",
        "    activation = ('Sigmoid{}'.format(i), nn.Sigmoid())\n",
        "    \n",
        "    # Append to decoder list\n",
        "    decoder.append(layer)\n",
        "    decoder.append(activation)\n",
        "    \n",
        "    # Wrap this in a container and declare the encoder/decoder\n",
        "    self.encoder = nn.Sequential(OrderedDict(encoder))\n",
        "    self.decoder = nn.Sequential(OrderedDict(decoder))\n",
        "    \n",
        "  def forward(self,x):\n",
        "    \n",
        "    # First encode the noisy image and then decode\n",
        "    x=self.encoder(x)\n",
        "    x=self.decoder(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJqcKjOyq1xn"
      },
      "source": [
        "def train(encoder_units, decoder_units, \n",
        "          epochs=100, batch_size=8, \n",
        "          input_dim = 784, output_dim = 784, \n",
        "          lr = 0.01, momentum= 0.09, weight_decay = 0,  \n",
        "          data_dir = '/content/drive/MyDrive/data'):\n",
        "    '''\n",
        "    This is the main training loop\n",
        "    '''\n",
        "    # Set device\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "    \n",
        "    # Set paths to datasets\n",
        "    paths = {\n",
        "        'train_imgs': os.path.join(data_dir, 'train-images-idx3-ubyte.gz'),\n",
        "        'train_labs': os.path.join(data_dir, 'train-labels-idx1-ubyte.gz'),\n",
        "        'test_imgs': os.path.join(data_dir,'t10k-images-idx3-ubyte.gz'),\n",
        "        'test_labs': os.path.join(data_dir,'t10k-labels-idx1-ubyte.gz')\n",
        "    }\n",
        "\n",
        "    # Store the quadrant definitions: move this into training loop later\n",
        "    quadrants = {\n",
        "      \n",
        "      1: [0, 0, 14, 14, 0], \n",
        "      2: [0, 14, 14, 14, 0],\n",
        "      3: [14, 0, 14, 14, 0],\n",
        "      4: [14, 14, 14, 14, 0],\n",
        "    }\n",
        "\n",
        "    # Load datasets\n",
        "    train_loader, test_loader = load_all_datasets(**paths, batch_size = batch_size)\n",
        "    \n",
        "    # Set parameters\n",
        "    net = DenoisingEncoder(encoder_units, decoder_units, input_dim, output_dim)\n",
        "    \n",
        "    # Send net object to device memory\n",
        "    net.to(device)\n",
        "    \n",
        "    # We use the cross-entropy loss\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # We use mini-batch stochastic gradient descent with momentum\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, \n",
        "                                            weight_decay=0)\n",
        "\n",
        "    # Store results here\n",
        "    results = {\n",
        "      'train_loss': [], \n",
        "      'val_loss': [], \n",
        "      }\n",
        "\n",
        "    # Loop over the dataset multiple times\n",
        "    for epoch in range(epochs):  \n",
        "        \n",
        "        # Initialize running loss\n",
        "        running_loss = 0.0\n",
        "        running_accuracy = 0.0\n",
        "\n",
        "        # Initialize the validation running loss\n",
        "        val_running_loss = 0.0\n",
        "        val_running_accuracy = 0.0\n",
        "        \n",
        "        # Iterate through data now\n",
        "        for i, data in enumerate(train_loader):\n",
        "            \n",
        "            # Get the inputs: data is a list of [inputs, labels]\n",
        "            clean_images, _ = data\n",
        "            \n",
        "            # Initialize container for noisy images\n",
        "            noisy_images = []\n",
        "\n",
        "            # Now get noisy images\n",
        "            for img in clean_images: \n",
        "              noisy_images.append(add_noise(img, quadrants))\n",
        "            \n",
        "            # Convert noisy image list to Torch tensor\n",
        "            noisy_images = torch.stack(noisy_images, dim =0)\n",
        "            \n",
        "            # Flatten noisy images\n",
        "            noisy_images=noisy_images.view(noisy_images.size(0),-1).type(torch.FloatTensor)\n",
        "            \n",
        "            # Flatten clean images\n",
        "            flat_clean_imgs = clean_images.view(clean_images.size(0),-1).type(torch.FloatTensor)\n",
        "\n",
        "            # Send the inputs and labels to the memory of the device\n",
        "            noisy_images, flat_clean_imgs = noisy_images.to(device), flat_clean_imgs.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            pred_images = net(noisy_images)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(pred_images, flat_clean_imgs)\n",
        "\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            \n",
        "            # Optimize\n",
        "            optimizer.step()\n",
        "\n",
        "            # Add to running loss\n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        # Loop through the validation data\n",
        "        for j, data in enumerate(test_loader):\n",
        "          \n",
        "          # No need to calculate gradients for validation set\n",
        "          with torch.no_grad():\n",
        "\n",
        "              # Get the inputs: data is a list of [inputs, labels]\n",
        "              val_clean_images, _ = data\n",
        "            \n",
        "              # Initialize container for noisy images\n",
        "              val_noisy_images = []\n",
        "\n",
        "              # Now get noisy images\n",
        "              # Flatten clean images for loss calculation\n",
        "              for img in val_clean_images: \n",
        "                val_noisy_images.append(add_noise(img, quadrants))\n",
        "            \n",
        "              # Convert noisy image list to Torch tensor\n",
        "              # Flatten noisy validation images\n",
        "              val_noisy_images = torch.stack(val_noisy_images, dim =0)\n",
        "              val_noisy_images=val_noisy_images.view(val_noisy_images.size(0),-1).type(torch.FloatTensor)\n",
        "              \n",
        "              # Flatten clean validation images\n",
        "              val_flat_clean_imgs = val_clean_images.view(val_clean_images.size(0),-1).type(torch.FloatTensor)\n",
        "\n",
        "              # Send the inputs and labels to the memory of the device\n",
        "              val_noisy_images, val_flat_clean_imgs = val_noisy_images.to(device), val_flat_clean_imgs.to(device)\n",
        "\n",
        "              # Send the data item through the network to get output\n",
        "              val_pred_images = net(val_noisy_images)\n",
        "\n",
        "              # Compute the loss\n",
        "              # Add to running loss\n",
        "              val_loss = criterion(val_pred_images, val_flat_clean_imgs)\n",
        "              val_running_loss += val_loss.item()\n",
        "        \n",
        "        # Rescale the training and validation perfomance metrics\n",
        "        running_loss = (running_loss*batch_size)/len(train_loader)\n",
        "        \n",
        "        # Rescale the validation loss\n",
        "        val_running_loss = (val_running_loss*batch_size)/len(test_loader)\n",
        "        \n",
        "        # Append to the results tracker\n",
        "        results['train_loss'].append(np.float(running_loss))\n",
        "        results['val_loss'].append(np.float(val_running_loss))\n",
        "\n",
        "        # Make print message format string\n",
        "        msg = \"Epoch:{} | Training Loss:{} | Validation Loss: {}\" \"\\n\"\n",
        "\n",
        "        # Print performance\n",
        "        print(msg.format(epoch, running_loss, val_running_loss))\n",
        "        \n",
        "    # Print message\n",
        "    print('Done training...')\n",
        "    \n",
        "    # Return statement\n",
        "    return(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCu5XWKpu0P5",
        "outputId": "702459aa-976b-4091-c578-e576d407c425"
      },
      "source": [
        "encoder_units = [512, 256, 128, 64]\n",
        "decoder_units = [128, 256, 512]\n",
        "lr = 0.7\n",
        "momentum = 0.99\n",
        "batch_size=64\n",
        "train(encoder_units = encoder_units, \n",
        "      decoder_units = decoder_units, \n",
        "      lr = lr, \n",
        "      momentum = momentum, \n",
        "      batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0 | Training Loss:4.400472933549617 | Validation Loss: 2.664925663334549\n",
            "\n",
            "Epoch:1 | Training Loss:2.3291591166941594 | Validation Loss: 2.1533975350628993\n",
            "\n",
            "Epoch:2 | Training Loss:1.9344924315969065 | Validation Loss: 1.8280896031932465\n",
            "\n",
            "Epoch:3 | Training Loss:1.7463820988435481 | Validation Loss: 1.6809110831303202\n",
            "\n",
            "Epoch:4 | Training Loss:1.6578196712902613 | Validation Loss: 1.617053161760804\n",
            "\n",
            "Epoch:5 | Training Loss:1.5753121121860008 | Validation Loss: 1.5708209572324328\n",
            "\n",
            "Epoch:6 | Training Loss:1.5230939613222314 | Validation Loss: 1.5320665244084255\n",
            "\n",
            "Epoch:7 | Training Loss:1.4892876296917767 | Validation Loss: 1.4726838852949202\n",
            "\n",
            "Epoch:8 | Training Loss:1.4504721077012102 | Validation Loss: 1.446104490832918\n",
            "\n",
            "Epoch:9 | Training Loss:1.4069776593495025 | Validation Loss: 1.4079469844793817\n",
            "\n",
            "Epoch:10 | Training Loss:1.3930564839194324 | Validation Loss: 1.3880532448458824\n",
            "\n",
            "Epoch:11 | Training Loss:1.3747770641404173 | Validation Loss: 1.369812119538617\n",
            "\n",
            "Epoch:12 | Training Loss:1.336608231957279 | Validation Loss: 1.339088768716071\n",
            "\n",
            "Epoch:13 | Training Loss:1.330042612323883 | Validation Loss: 1.327123063385107\n",
            "\n",
            "Epoch:14 | Training Loss:1.3075802820577804 | Validation Loss: 1.2948599245138228\n",
            "\n",
            "Epoch:15 | Training Loss:1.2880706707678877 | Validation Loss: 1.3167175866995648\n",
            "\n",
            "Epoch:16 | Training Loss:1.2711993126726862 | Validation Loss: 1.2871894153060428\n",
            "\n",
            "Epoch:17 | Training Loss:1.253846832175753 | Validation Loss: 1.28629783612148\n",
            "\n",
            "Epoch:18 | Training Loss:1.2471241117921719 | Validation Loss: 1.264777730984293\n",
            "\n",
            "Epoch:19 | Training Loss:1.2454916231794906 | Validation Loss: 1.2489141066362903\n",
            "\n",
            "Epoch:20 | Training Loss:1.2276326776948818 | Validation Loss: 1.2462491290584492\n",
            "\n",
            "Epoch:21 | Training Loss:1.216615198517659 | Validation Loss: 1.2336431935334662\n",
            "\n",
            "Epoch:22 | Training Loss:1.1992682273835262 | Validation Loss: 1.2213008441742819\n",
            "\n",
            "Epoch:23 | Training Loss:1.194727134602919 | Validation Loss: 1.2085653421985116\n",
            "\n",
            "Epoch:24 | Training Loss:1.1999855635008578 | Validation Loss: 1.219492301060136\n",
            "\n",
            "Epoch:25 | Training Loss:1.2032718612059856 | Validation Loss: 1.23242797631367\n",
            "\n",
            "Epoch:26 | Training Loss:1.1818903061880994 | Validation Loss: 1.2027098752890424\n",
            "\n",
            "Epoch:27 | Training Loss:1.1727543294048512 | Validation Loss: 1.1967328000979818\n",
            "\n",
            "Epoch:28 | Training Loss:1.1667524077999059 | Validation Loss: 1.2025691711219253\n",
            "\n",
            "Epoch:29 | Training Loss:1.167631498150734 | Validation Loss: 1.1853776038831967\n",
            "\n",
            "Epoch:30 | Training Loss:1.1654906853048532 | Validation Loss: 1.179886327807311\n",
            "\n",
            "Epoch:31 | Training Loss:1.1543885955551285 | Validation Loss: 1.1819874222870845\n",
            "\n",
            "Epoch:32 | Training Loss:1.147919415410902 | Validation Loss: 1.1595998991067242\n",
            "\n",
            "Epoch:33 | Training Loss:1.137895145777192 | Validation Loss: 1.1617642975157234\n",
            "\n",
            "Epoch:34 | Training Loss:1.1226476450591707 | Validation Loss: 1.1515855056465052\n",
            "\n",
            "Epoch:35 | Training Loss:1.128743597566446 | Validation Loss: 1.1513587323723324\n",
            "\n",
            "Epoch:36 | Training Loss:1.1261036674351073 | Validation Loss: 1.1828679036182963\n",
            "\n",
            "Epoch:37 | Training Loss:1.1322430649927175 | Validation Loss: 1.1392668375543729\n",
            "\n",
            "Epoch:38 | Training Loss:1.1016626389168982 | Validation Loss: 1.1370549972649593\n",
            "\n",
            "Epoch:39 | Training Loss:1.091081514986339 | Validation Loss: 1.1234972252967252\n",
            "\n",
            "Epoch:40 | Training Loss:1.1015383614532983 | Validation Loss: 1.1484671390739976\n",
            "\n",
            "Epoch:41 | Training Loss:1.0951728832238772 | Validation Loss: 1.109440522588742\n",
            "\n",
            "Epoch:42 | Training Loss:1.087632057763366 | Validation Loss: 1.1340173312053559\n",
            "\n",
            "Epoch:43 | Training Loss:1.0857154800693618 | Validation Loss: 1.1134010644475365\n",
            "\n",
            "Epoch:44 | Training Loss:1.0789207442482907 | Validation Loss: 1.1206631588328415\n",
            "\n",
            "Epoch:45 | Training Loss:1.0644969121098264 | Validation Loss: 1.106472795176658\n",
            "\n",
            "Epoch:46 | Training Loss:1.0705689107304188 | Validation Loss: 1.1046338707778105\n",
            "\n",
            "Epoch:47 | Training Loss:1.0758949970004401 | Validation Loss: 1.1126806728399483\n",
            "\n",
            "Epoch:48 | Training Loss:1.0755630146974184 | Validation Loss: 1.1084945976354514\n",
            "\n",
            "Epoch:49 | Training Loss:1.0656507709768535 | Validation Loss: 1.1111139787989817\n",
            "\n",
            "Epoch:50 | Training Loss:1.057249630247352 | Validation Loss: 1.0921252984909495\n",
            "\n",
            "Epoch:51 | Training Loss:1.07344550936461 | Validation Loss: 1.0920238278474017\n",
            "\n",
            "Epoch:52 | Training Loss:1.0686926161175343 | Validation Loss: 1.1043627884737246\n",
            "\n",
            "Epoch:53 | Training Loss:1.0603225202575675 | Validation Loss: 1.1124097454320094\n",
            "\n",
            "Epoch:54 | Training Loss:1.0594648082754505 | Validation Loss: 1.0908469140149986\n",
            "\n",
            "Epoch:55 | Training Loss:1.0411120741479178 | Validation Loss: 1.0821886176516295\n",
            "\n",
            "Epoch:56 | Training Loss:1.0495835521709183 | Validation Loss: 1.07608233667483\n",
            "\n",
            "Epoch:57 | Training Loss:1.0405827571334108 | Validation Loss: 1.0825458294267107\n",
            "\n",
            "Epoch:58 | Training Loss:1.040495944683994 | Validation Loss: 1.1050590151434492\n",
            "\n",
            "Epoch:59 | Training Loss:1.0516740499910262 | Validation Loss: 1.0948159189740563\n",
            "\n",
            "Epoch:60 | Training Loss:1.047083059671337 | Validation Loss: 1.079669641461342\n",
            "\n",
            "Epoch:61 | Training Loss:1.03793409040996 | Validation Loss: 1.0878174601087145\n",
            "\n",
            "Epoch:62 | Training Loss:1.0383769547634287 | Validation Loss: 1.0789681771758255\n",
            "\n",
            "Epoch:63 | Training Loss:1.0331331657956657 | Validation Loss: 1.0810217519474636\n",
            "\n",
            "Epoch:64 | Training Loss:1.0299972436194227 | Validation Loss: 1.0546673114892025\n",
            "\n",
            "Epoch:65 | Training Loss:1.0248395259827694 | Validation Loss: 1.0713619654345665\n",
            "\n",
            "Epoch:66 | Training Loss:1.0386386354213584 | Validation Loss: 1.0676451974613652\n",
            "\n",
            "Epoch:67 | Training Loss:1.0206307842533218 | Validation Loss: 1.0576297876181875\n",
            "\n",
            "Epoch:68 | Training Loss:1.0239124850296517 | Validation Loss: 1.0673240316901238\n",
            "\n",
            "Epoch:69 | Training Loss:1.0221217697871519 | Validation Loss: 1.0826256521947824\n",
            "\n",
            "Epoch:70 | Training Loss:1.0250914886689135 | Validation Loss: 1.068477358408035\n",
            "\n",
            "Epoch:71 | Training Loss:1.0256657127632516 | Validation Loss: 1.059787387301208\n",
            "\n",
            "Epoch:72 | Training Loss:1.0295217678999342 | Validation Loss: 1.0742897042043649\n",
            "\n",
            "Epoch:73 | Training Loss:1.0175113941687766 | Validation Loss: 1.0476792454719543\n",
            "\n",
            "Epoch:74 | Training Loss:1.0134887966647077 | Validation Loss: 1.0638879621104829\n",
            "\n",
            "Epoch:75 | Training Loss:1.0112355456296316 | Validation Loss: 1.0526666857634381\n",
            "\n",
            "Epoch:76 | Training Loss:1.0168044331993884 | Validation Loss: 1.0573622409705143\n",
            "\n",
            "Epoch:77 | Training Loss:1.0194027054665693 | Validation Loss: 1.050919970509353\n",
            "\n",
            "Epoch:78 | Training Loss:1.0130606218695895 | Validation Loss: 1.0461296344258983\n",
            "\n",
            "Epoch:79 | Training Loss:1.006735820188197 | Validation Loss: 1.0657399744744513\n",
            "\n",
            "Epoch:80 | Training Loss:1.0054733371302518 | Validation Loss: 1.0496641704990606\n",
            "\n",
            "Epoch:81 | Training Loss:1.006977555657755 | Validation Loss: 1.0478211683072862\n",
            "\n",
            "Epoch:82 | Training Loss:1.0189113093972968 | Validation Loss: 1.0578767771174193\n",
            "\n",
            "Epoch:83 | Training Loss:1.0105466871246347 | Validation Loss: 1.0587449005454967\n",
            "\n",
            "Epoch:84 | Training Loss:1.01053154824385 | Validation Loss: 1.054841418934476\n",
            "\n",
            "Epoch:85 | Training Loss:1.0012381094605176 | Validation Loss: 1.0503474010783396\n",
            "\n",
            "Epoch:86 | Training Loss:1.004700721898821 | Validation Loss: 1.0490129274927127\n",
            "\n",
            "Epoch:87 | Training Loss:0.9920806887307401 | Validation Loss: 1.0433302381236083\n",
            "\n",
            "Epoch:88 | Training Loss:1.0015339359545758 | Validation Loss: 1.0631133560921735\n",
            "\n",
            "Epoch:89 | Training Loss:1.003087563746011 | Validation Loss: 1.0469708511024525\n",
            "\n",
            "Epoch:90 | Training Loss:0.9975921868769599 | Validation Loss: 1.0535957346296614\n",
            "\n",
            "Epoch:91 | Training Loss:0.9973889293828244 | Validation Loss: 1.042068642415818\n",
            "\n",
            "Epoch:92 | Training Loss:0.9993595897770131 | Validation Loss: 1.070278031431186\n",
            "\n",
            "Epoch:93 | Training Loss:1.0048091580618674 | Validation Loss: 1.0512089702733762\n",
            "\n",
            "Epoch:94 | Training Loss:1.0069351838087477 | Validation Loss: 1.0669362761412458\n",
            "\n",
            "Epoch:95 | Training Loss:1.0007245778274942 | Validation Loss: 1.039448476521073\n",
            "\n",
            "Epoch:96 | Training Loss:0.9988403689505448 | Validation Loss: 1.0452406527889762\n",
            "\n",
            "Epoch:97 | Training Loss:0.9930496373410418 | Validation Loss: 1.0306746823013209\n",
            "\n",
            "Epoch:98 | Training Loss:0.993424943578777 | Validation Loss: 1.0401718829088151\n",
            "\n",
            "Epoch:99 | Training Loss:0.9866011488412235 | Validation Loss: 1.0392973552084273\n",
            "\n",
            "Done training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_loss': [4.400472933549617,\n",
              "  2.3291591166941594,\n",
              "  1.9344924315969065,\n",
              "  1.7463820988435481,\n",
              "  1.6578196712902613,\n",
              "  1.5753121121860008,\n",
              "  1.5230939613222314,\n",
              "  1.4892876296917767,\n",
              "  1.4504721077012102,\n",
              "  1.4069776593495025,\n",
              "  1.3930564839194324,\n",
              "  1.3747770641404173,\n",
              "  1.336608231957279,\n",
              "  1.330042612323883,\n",
              "  1.3075802820577804,\n",
              "  1.2880706707678877,\n",
              "  1.2711993126726862,\n",
              "  1.253846832175753,\n",
              "  1.2471241117921719,\n",
              "  1.2454916231794906,\n",
              "  1.2276326776948818,\n",
              "  1.216615198517659,\n",
              "  1.1992682273835262,\n",
              "  1.194727134602919,\n",
              "  1.1999855635008578,\n",
              "  1.2032718612059856,\n",
              "  1.1818903061880994,\n",
              "  1.1727543294048512,\n",
              "  1.1667524077999059,\n",
              "  1.167631498150734,\n",
              "  1.1654906853048532,\n",
              "  1.1543885955551285,\n",
              "  1.147919415410902,\n",
              "  1.137895145777192,\n",
              "  1.1226476450591707,\n",
              "  1.128743597566446,\n",
              "  1.1261036674351073,\n",
              "  1.1322430649927175,\n",
              "  1.1016626389168982,\n",
              "  1.091081514986339,\n",
              "  1.1015383614532983,\n",
              "  1.0951728832238772,\n",
              "  1.087632057763366,\n",
              "  1.0857154800693618,\n",
              "  1.0789207442482907,\n",
              "  1.0644969121098264,\n",
              "  1.0705689107304188,\n",
              "  1.0758949970004401,\n",
              "  1.0755630146974184,\n",
              "  1.0656507709768535,\n",
              "  1.057249630247352,\n",
              "  1.07344550936461,\n",
              "  1.0686926161175343,\n",
              "  1.0603225202575675,\n",
              "  1.0594648082754505,\n",
              "  1.0411120741479178,\n",
              "  1.0495835521709183,\n",
              "  1.0405827571334108,\n",
              "  1.040495944683994,\n",
              "  1.0516740499910262,\n",
              "  1.047083059671337,\n",
              "  1.03793409040996,\n",
              "  1.0383769547634287,\n",
              "  1.0331331657956657,\n",
              "  1.0299972436194227,\n",
              "  1.0248395259827694,\n",
              "  1.0386386354213584,\n",
              "  1.0206307842533218,\n",
              "  1.0239124850296517,\n",
              "  1.0221217697871519,\n",
              "  1.0250914886689135,\n",
              "  1.0256657127632516,\n",
              "  1.0295217678999342,\n",
              "  1.0175113941687766,\n",
              "  1.0134887966647077,\n",
              "  1.0112355456296316,\n",
              "  1.0168044331993884,\n",
              "  1.0194027054665693,\n",
              "  1.0130606218695895,\n",
              "  1.006735820188197,\n",
              "  1.0054733371302518,\n",
              "  1.006977555657755,\n",
              "  1.0189113093972968,\n",
              "  1.0105466871246347,\n",
              "  1.01053154824385,\n",
              "  1.0012381094605176,\n",
              "  1.004700721898821,\n",
              "  0.9920806887307401,\n",
              "  1.0015339359545758,\n",
              "  1.003087563746011,\n",
              "  0.9975921868769599,\n",
              "  0.9973889293828244,\n",
              "  0.9993595897770131,\n",
              "  1.0048091580618674,\n",
              "  1.0069351838087477,\n",
              "  1.0007245778274942,\n",
              "  0.9988403689505448,\n",
              "  0.9930496373410418,\n",
              "  0.993424943578777,\n",
              "  0.9866011488412235],\n",
              " 'val_loss': [2.664925663334549,\n",
              "  2.1533975350628993,\n",
              "  1.8280896031932465,\n",
              "  1.6809110831303202,\n",
              "  1.617053161760804,\n",
              "  1.5708209572324328,\n",
              "  1.5320665244084255,\n",
              "  1.4726838852949202,\n",
              "  1.446104490832918,\n",
              "  1.4079469844793817,\n",
              "  1.3880532448458824,\n",
              "  1.369812119538617,\n",
              "  1.339088768716071,\n",
              "  1.327123063385107,\n",
              "  1.2948599245138228,\n",
              "  1.3167175866995648,\n",
              "  1.2871894153060428,\n",
              "  1.28629783612148,\n",
              "  1.264777730984293,\n",
              "  1.2489141066362903,\n",
              "  1.2462491290584492,\n",
              "  1.2336431935334662,\n",
              "  1.2213008441742819,\n",
              "  1.2085653421985116,\n",
              "  1.219492301060136,\n",
              "  1.23242797631367,\n",
              "  1.2027098752890424,\n",
              "  1.1967328000979818,\n",
              "  1.2025691711219253,\n",
              "  1.1853776038831967,\n",
              "  1.179886327807311,\n",
              "  1.1819874222870845,\n",
              "  1.1595998991067242,\n",
              "  1.1617642975157234,\n",
              "  1.1515855056465052,\n",
              "  1.1513587323723324,\n",
              "  1.1828679036182963,\n",
              "  1.1392668375543729,\n",
              "  1.1370549972649593,\n",
              "  1.1234972252967252,\n",
              "  1.1484671390739976,\n",
              "  1.109440522588742,\n",
              "  1.1340173312053559,\n",
              "  1.1134010644475365,\n",
              "  1.1206631588328415,\n",
              "  1.106472795176658,\n",
              "  1.1046338707778105,\n",
              "  1.1126806728399483,\n",
              "  1.1084945976354514,\n",
              "  1.1111139787989817,\n",
              "  1.0921252984909495,\n",
              "  1.0920238278474017,\n",
              "  1.1043627884737246,\n",
              "  1.1124097454320094,\n",
              "  1.0908469140149986,\n",
              "  1.0821886176516295,\n",
              "  1.07608233667483,\n",
              "  1.0825458294267107,\n",
              "  1.1050590151434492,\n",
              "  1.0948159189740563,\n",
              "  1.079669641461342,\n",
              "  1.0878174601087145,\n",
              "  1.0789681771758255,\n",
              "  1.0810217519474636,\n",
              "  1.0546673114892025,\n",
              "  1.0713619654345665,\n",
              "  1.0676451974613652,\n",
              "  1.0576297876181875,\n",
              "  1.0673240316901238,\n",
              "  1.0826256521947824,\n",
              "  1.068477358408035,\n",
              "  1.059787387301208,\n",
              "  1.0742897042043649,\n",
              "  1.0476792454719543,\n",
              "  1.0638879621104829,\n",
              "  1.0526666857634381,\n",
              "  1.0573622409705143,\n",
              "  1.050919970509353,\n",
              "  1.0461296344258983,\n",
              "  1.0657399744744513,\n",
              "  1.0496641704990606,\n",
              "  1.0478211683072862,\n",
              "  1.0578767771174193,\n",
              "  1.0587449005454967,\n",
              "  1.054841418934476,\n",
              "  1.0503474010783396,\n",
              "  1.0490129274927127,\n",
              "  1.0433302381236083,\n",
              "  1.0631133560921735,\n",
              "  1.0469708511024525,\n",
              "  1.0535957346296614,\n",
              "  1.042068642415818,\n",
              "  1.070278031431186,\n",
              "  1.0512089702733762,\n",
              "  1.0669362761412458,\n",
              "  1.039448476521073,\n",
              "  1.0452406527889762,\n",
              "  1.0306746823013209,\n",
              "  1.0401718829088151,\n",
              "  1.0392973552084273]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    }
  ]
}